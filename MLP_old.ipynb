{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron for Optical Character Recognition\n",
    "\n",
    "Amitesh Sahu 21074006 \n",
    "\n",
    "Rituraj Barai 21074024\n",
    "\n",
    "Soustab Haldar 21074029\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:16:15.682784Z",
     "start_time": "2019-07-15T22:16:15.559468Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "One of the first steps when working with a new data set is preprocessing.  Our data here is from our dataset  of `28x28` black & white pixel images of handwritten preprocessed lowercase letters from `a-z`.\n",
    "The dataset is in the form of folders for each of the alphabet where the folder name is the label for an image. For MLP, the image data has to be converted into numerical sing pillow library and python lists. Also, the array of image data should be appended with an appropriate label. The label alphabets are comverted into numerical values using a disctionary. Then the images are stored in the form of a list of image data lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "c\n",
      "d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\n",
      "f\n",
      "g\n",
      "h\n",
      "i\n",
      "j\n",
      "k\n",
      "l\n",
      "m\n",
      "n\n",
      "o\n",
      "p\n",
      "q\n",
      "r\n",
      "s\n",
      "t\n",
      "u\n",
      "v\n",
      "w\n",
      "x\n",
      "y\n",
      "z\n"
     ]
    }
   ],
   "source": [
    "root_dir=r\"D:\\IIT BHU\\SEM V\\IC\\mlp-implementation\\imgs\\\\\"\n",
    "dict = {chr(i): i - ord('a') + 1 for i in range(ord('a'), ord('z') + 1)}\n",
    "lis = []\n",
    "for i in os.listdir(root_dir):\n",
    "    path=os.path.join(root_dir,i)\n",
    "    for j in os.listdir(path):\n",
    "        img_path=os.path.join(path,j)\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        l=[dict[i]]\n",
    "        li=list(img.getdata())\n",
    "        l=l+li\n",
    "        lis+=[l]\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we wrtie the image lists into a CSV file for easy access in further uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir=r\"D:\\IIT BHU\\SEM V\\IC\\mlp-implementation\\data\\data.csv\"\n",
    "with open(csv_dir, 'w',newline='') as csvfile:   \n",
    "    csvwriter = csv.writer(csvfile)  \n",
    "    csvwriter.writerows(lis) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data from the CSV file and converting it into a list of image data lists and a list of labels as X and Y values for the MLP model respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:16:16.437154Z",
     "start_time": "2019-07-15T22:16:16.431149Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_data(dir_name):\n",
    "    \"\"\"\n",
    "    Function for loading data stored in comma delimited files. Labels for \n",
    "    each image are the first entry in each row.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dit_name : str\n",
    "         Path to where data is contained\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : array_like\n",
    "        A (N x p=784) matrix of samples \n",
    "    Y : array_like\n",
    "        A (N x 1) matrix of labels for each sample\n",
    "    \"\"\"\n",
    "    data = list() # init a list called `data`\n",
    "    \n",
    "    with open(dir_name,\"r\") as f: # open the directory as a read (\"r\"), call it `f`\n",
    "        for line in f: # iterate through each `line` in `f`\n",
    "            split_line = np.array(line.split(',')) # split lines by `,` - cast the resultant list into an numpy array\n",
    "            split_line = split_line.astype(np.float32) # make the numpy array of str into floats\n",
    "            data.append(split_line) # collect the sample into the `data` list\n",
    "            \n",
    "    data = np.asarray(data) # convert the `data` list into a numpy array for easier indexing\n",
    "    \n",
    "    # as the first number in each sample is the label (0-9), extract that from the rest and return both (X,Y)\n",
    "    return data[:,1:],data[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataread from the CSV file has to be split into training and testing values. The training data is used to train the model and the testing data is used to test the model. The data is split into 80% training and 20% testing data. Using random state ensures that the splitting is completely random and does not follow any set pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:10:03.705155Z",
     "start_time": "2019-07-15T20:09:44.341375Z"
    }
   },
   "outputs": [],
   "source": [
    "X_data,Y_data = load_data(\"data/data.csv\")\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X_data,Y_data,test_size=0.2,random_state=104)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `.shape` attribute of numpy arrays. We see that the shape of `X_train` is `5933 x 784`, thus there are `5933` samples (images) each with dimension `784`. Each sample, typically presented as a 28 x 28 image, is unrolled into a 1-dimensional vector 28 x 28 = 784 contained within each row of `X_train`. Also shape of `X_train` is `1484 X 784`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:10:03.720141Z",
     "start_time": "2019-07-15T20:10:03.717400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training set is: 5933 x 784\n",
      "The shape of the test set is: 1484 x 784\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of the training set is: {X_train.shape[0]} x {X_train.shape[1]}\")\n",
    "print(f\"The shape of the test set is: {X_test.shape[0]} x {X_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing the data samples. We can see that `Y_train[index]` is `3.0` which means it represents the letter `c`. Subsequently the `X_train[index]` is the array of the values of all the pixels in that image between `0` and `255` brightness levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:10:03.766216Z",
     "start_time": "2019-07-15T20:10:03.734027Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.0,\n",
       " array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   1.,   1.,   1.,   1.,   1.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,\n",
       "          3.,   2.,   0.,   0.,   0.,   0.,   0.,   1.,   1.,   1.,   1.,\n",
       "          1.,   1.,   1.,   1.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   1.,   1.,\n",
       "          1.,   1.,   1.,   1.,   1.,   1.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
       "          1.,   0.,   0.,   0.,  44., 103., 162., 162., 131.,  54.,   3.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   1.,\n",
       "          1.,   1.,   1.,   1.,   1.,   1.,   0.,   0.,   0.,  94., 245.,\n",
       "        255., 162., 162., 189., 255., 149.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   0.,\n",
       "          0.,   0.,  19., 198., 206.,  85.,  15.,   0.,   0.,   4.,  57.,\n",
       "        217.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,\n",
       "          0.,   0.,   1.,   1.,   1.,   0.,   0.,   0., 160., 200.,  25.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   1.,   1.,\n",
       "          0.,   0.,  87., 247.,  60.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   1.,   1.,   0.,  45., 237., 113.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0., 163., 226.,   6.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          1.,   0.,   0.,   0.,   0.,   0.,   0.,  24., 222.,  99.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   1.,   1.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0., 173., 243.,  28.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
       "          1.,   1.,   0.,   0.,   0.,   0.,   0.,   0., 173., 173.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,\n",
       "          1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0., 173., 120.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   1.,\n",
       "          1.,   1.,   1.,   0.,   0.,   0.,   0.,   0.,   0., 173.,  79.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0., 173., 173.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 166.,\n",
       "        213.,  23.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,  21., 194., 216., 103.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,  51.,  54.,  54.,  54.,  31.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,  17.,  99., 249., 201., 196., 118.,  93.,  93., 118., 196.,\n",
       "        201., 252., 255., 255., 255., 106.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  22., 124.,\n",
       "        124., 124., 124., 124., 124., 124., 124.,  23.,  15.,  15.,  15.,\n",
       "          2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.], dtype=float32))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "Y_train[index], X_train[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is nothing in principle wrong with this `0-255` representation, the value of `255` being the maximum is specific to images and somewhat an arbitrary for our purposes. Frequently people opt to rescale their data to range between `0-1`, which will be have some nice mathematical properties for us later. Of course, this can be done by simply dividing each entry in `X_train` and `X_test` by its maximum value (accessed using `X_train.max()`). \n",
    "\n",
    "Further, each label in `Y_train` and `Y_test` are currently alphabets converted to integers (e.g. `25.0` or `2.0`). For categorical data (where image is labeled between `0-26`) we opt for a one-hot encoded representation of our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:10:03.883943Z",
     "start_time": "2019-07-15T20:10:03.780726Z"
    }
   },
   "outputs": [],
   "source": [
    "# rescale data between 0 - 1.0\n",
    "X_train = X_train/X_train.max()\n",
    "X_test = X_test/X_test.max()\n",
    "\n",
    "# one-hot encode train (y_train) and test (y_test) set labels\n",
    "y_train = np.zeros((Y_train.size, int(Y_train.max()) + 1))\n",
    "y_train[np.arange(Y_train.size),Y_train.astype(int)] = 1.0\n",
    "\n",
    "y_test = np.zeros((Y_test.size, int(Y_test.max()) + 1))\n",
    "y_test[np.arange(Y_test.size),Y_test.astype(int)] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:10:04.313194Z",
     "start_time": "2019-07-15T20:10:03.897501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_images = 3\n",
    "fig,axes = plt.subplots(1,num_images,figsize=(15,10))\n",
    "for image,label,ax in zip(X_train[:num_images],y_train[:num_images],axes):\n",
    "    ax.imshow(image.reshape(28,28),cmap='gray',vmin=0,vmax=1.0)\n",
    "    for i in range(len(label)):\n",
    "        if(label[i]==1.0):\n",
    "            lab=list(dict.keys())[list(dict.values()).index(i)]\n",
    "    ax.set_title(f\"Label: {lab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image above represents schematically the conventional perceptron. Each input, represented as a multidimensional vector $\\mathbf{x}=\\{x_0=1,x_1,x_2,\\ldots,x_n\\}$, is multiplied by a set of weights $\\mathbf{w} = \\{w_0,w_1,w_2,\\ldots,w_n\\}$ to produce the weighted sum $$z = \\sum_{i=0}^{n} x_i w_i$$. In the case of a vanilla perceptron, this weighted sum $z$ is lastly fed through as step function to produce the predicted output $a = \\text{step}(z)$. This results in the output where $a=1$ if $z>0$ and $a=0$ if $z<0$.\n",
    "\n",
    "With the output $a$ taking the value 0 or 1, this type of structure really only works for binary classification problems (i.e. situations where each input $\\mathbf{x}$ has one of two intended labels: 0 or 1). The weights $\\mathbf{w}$ in this context can be interpreted as assigning credence to the different inputs $x_i$ based on their relative importance. Notice, there is an additional constant $x_0=1$ concatenated to each input, frequently called the *bias term* or just *the bias*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These neural networks are often schematically represented with a series of nodes and arrows as shown on the right. Note, in our application to MNIST each input would be `784+1` dimensional (including the bias) and the output `26` dimensional for representing the one-hot encoded binary vector of output labels. Rather than restricting ourselves to treating each sum with a step function as in the vanilla perceptron, this concept is generalized to any arbitrary function typically called the *activation function*, depicted as $\\sigma(\\cdot)$ in the schematic. In principle almost any function may be an activation function, however people typically use one of the following activation functions due to some nice mathematical properties we will discuss later:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed forward\n",
    "Given a predefined neural network *architecture* (the *architecture* of a neural network refers to all the elements necessary to completely define the flow of data, which involve the number and size of hidden layers, which activation functions, the output size, etc.) the process of generating an output from an input is called a *forward* pass. As we shall see, for an MLP the forward pass may be succinctly represented as a series of matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the MLP represented schematically above with sigmoid activations $\\sigma$ in the hidden layer. Each neuron in the hidden layers will be weighted sums of the inputs: $$a_j = \\sigma(\\sum_{i=1}^{784} x_i* w_{i,j})$$ for $j=\\{1,2,\\ldots,15\\}$ (notice, there is no bias term in this example). From here it clear to see that the weights $w_{i,j}$ may be compacted into a matrix $W \\in {\\rm I\\!R}^{784\\times15}$ where $W_{i,j} = w_{i,j}$, allowing for all the neurons in the hidden layer to be efficiently calculated using matrix multiplication: $\\mathbf{a}^{(1)} = \\sigma(\\mathbf{z}^{(1)}) = \\sigma(\\mathbf{x}W)$. Where $\\mathbf{a}^{(i)},\\mathbf{z}^{(i)} \\in {\\rm I\\!R}^{1\\times 15}$, with the superscript $(i)$ indicating the assocaited layer number, and $\\mathbf{x} \\in {\\rm I\\!R}^{1\\times 784}$ are both arranged as column vectors. Another, different, weight matrix is needed to transform the hidden layer to the output layer. Let's demarcate these two as $W^{(1)}$ for the matrix which transforms the inputs to the hidden layer $\\mathbf{a}^{(1)}$ and $W^{(2)} \\in {\\rm I\\!R}^{15\\times 10}$ for transforming the hidden layer to the output layer $\\mathbf{z}^{(2)}$. Mathematically, $$\\mathbf{z}^{(2)} = \\sigma(\\mathbf{x}W^{(1)})W^{(2)}$$. Notice, we've yet to treat this output $\\mathbf{z}^{(2)}$ with an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:32:36.274844Z",
     "start_time": "2019-07-15T22:32:36.261999Z"
    }
   },
   "source": [
    "The last step of process for classification tasks is actually producing a prediction from these numbers in the output layer $\\mathbf{z}^{(2)}$. Typically, in the case of multi-label classification, this is done using a *softmax* activation function which effectively converts the output neurons into probabilities for each label. This has the mathematical form, $$\\text{softmax}(\\mathbf{z})_j = \\frac{\\exp(z_j)}{\\sum_{k=0}^{K=9}\\exp{(z_k)}}$$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "Now that we have an understanding of how an MLP generates outputs from inputs, we engage the problem of how to actually *train* this neural network. *Training* a neural network (in a supervised setting, which means each input comes with a known output) refers to the process of iteratively updating the weights of the network to improve it's performance. The performance of the neural network is evaluated using a *loss function* which quantitatively measures how \"close\" the neural network output is to the true output. In short, using *backpropagation* we aim to minimize the loss function with respect to the weights (also called *parameters*) of the neural network.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer perceptron: the code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the forward pass of the neural network is relatively simply as everything is basically just a series of matrix multiplicaitons. The example we looked at when working through the math had a forward pass which mathematically took the form \n",
    "$\\text{softmax}((\\sigma(\\mathbf{x}W^{(1)})W^{(2)}))$ .\n",
    "We'll of course want to make things a bit more modular by allowing the user to choose some of the neural network paramters, such as the number of hidden layers `N_l` and the nunber of neurons per layer `L`. For simplicity we'll restrict that all the hidden layers are treated with sigmoid activaitons and the final layer processed with a softmax activation, which if you recall will be nessesary for us to ultimately purpose this MLP to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by simply defining our activation functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs `x`, weight matricies `w`, and activations are in principle all we need to define the forward pass; however, for efficency reasons we'll want to store the outputs of the hidden layer neurons when performing the forward pass. Storing these values will help us later more quickly calculate the gradients during the backward pass. The `init_layers` functions will initalize these hidden layers as NumPy arrays, doing this before we begin training will help us save some overhead we would otherwise inccur reinitalizing these hidden layers before each forward pass. These hidden layer values will be stored in multi-dimensional matricies, called *tensors*. One dimension of these tensors will be the `batch size` which will indicate the number of samples simultaneously passed to MLP during one training loop (feed forward + backpropagation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform a forward pass our input `x` is consequtively multiplied by weight matricies passed into the associated activaiton functions. The paramters in these weight matricies will ultimately be learned through backpropagation, but each weight matrix must first be initalized to random values. There are a number of different methods for doing this initalization, but for the moment we'll use a simple approach of just drawing the numerical values from a normal distribution with mean zero and standard deviation 1. We could have also reasonably choosen to simply draw from a uniform distribution on the range `[-1,1]` (why would it be wrong to initalize the weight matricies with all zeros?). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the `feed_forward` function to iterate though the calculations to perform the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalizing all the weights, layers, and activations prior to the forward pass makes much of the backward pass implimentation actually quite simple.\n",
    "\n",
    "We define a `sigmoid_prime` function, which simply computes the derivative of the sigmoid activation $\\sigma^{\\prime}$. We'll use this when computing the gradients during the backward pass. Recall, $\\sigma^{\\prime}(x) = \\sigma(x)(1-\\sigma(x))$.\n",
    "\n",
    "We define the `tanh_prime` function, which simply computes the derivative of the tanh activation $\\sigma^{\\prime}$. We'll use this when computing the gradients during the backward pass. Recall, $\\sigma^{\\prime}(x) = 1-\\sigma(x)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of `epochs` we set defines the number of iterations we train our model. In each `epoch` a number of forward + backward pass are iteratively performed on mini-batches of data until our neural network has *seen* all the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron: Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    \n",
    "    def __init__(self,X,Y,X_val,Y_val,L=1,N_l=128):\n",
    "        self.X = np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n",
    "        self.Y = np.squeeze(np.eye(27)[Y.astype(int).reshape(-1)])\n",
    "        self.X_val = np.concatenate((X_val,np.ones((X_val.shape[0],1))),axis=1)\n",
    "        self.Y_val = np.squeeze(np.eye(27)[Y_val.astype(int).reshape(-1)])\n",
    "        self.L = L\n",
    "        self.N_l = N_l\n",
    "        self.n_samples = self.X.shape[0]\n",
    "        self.layer_sizes =np.array([self.X.shape[1]]+[N_l]*L+[self.Y.shape[1]]) \n",
    "        self.__init_weights()\n",
    "        self.train_loss = list()\n",
    "        self.train_acc = list()\n",
    "        self.val_loss = list()\n",
    "        self.val_acc = list()\n",
    "        self.train_time = list()\n",
    "        self.tot_time = list()\n",
    "        self.metrics = [self.train_loss,self.train_acc,self.val_loss,self.val_acc,self.train_time,self.tot_time]\n",
    "        \n",
    "    def __sigmoid(self,x):\n",
    "        # VCompute the sigmoid\n",
    "        return 1./(1.+np.exp(-x))\n",
    "\n",
    "    def __tanh(self,x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def __softmax(self,x):\n",
    "        \"\"\"\n",
    "    Compute the softmax of `x`,\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        (N x dim) array with N samples by p dimensions. dim=10 for MNIST classification. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    softmax(x) : float or array_like\n",
    "        softmax applied to `x` along the first axis.\n",
    "    \"\"\"\n",
    "        # Compute softmax along the rows of the input\n",
    "        exponent = np.exp(x)\n",
    "        return exponent/exponent.sum(axis=1,keepdims=True)\n",
    "    \n",
    "    def __loss(self,y_pred,y):\n",
    "        # Compute the loss along the rows, averaging along the number of samples\n",
    "        return ((-np.log(y_pred))*y).sum(axis=1).mean()\n",
    "    \n",
    "    def __accuracy(self,y_pred,y):  \n",
    "        # Compute the accuracy along the rows, averaging along the number of samples\n",
    "        return np.all(y_pred==y,axis=1).mean()\n",
    "    \n",
    "    def __sigmoid_prime(self,h):\n",
    "        # Compute the derivative of sigmoid where h=sigmoid(x)\n",
    "        return h*(1-h)\n",
    "    \n",
    "    def __tanh_prime(self,h):\n",
    "        return 1-h**2\n",
    "    \n",
    "    def __to_categorical(self,x):  \n",
    "        # Transform probabilities into categorical predictions row-wise, by simply taking the max probability\n",
    "        categorical = np.zeros((x.shape[0],self.Y.shape[1]))\n",
    "        categorical[np.arange(x.shape[0]),x.argmax(axis=1)] = 1\n",
    "        return categorical\n",
    "    \n",
    "    def __init_weights(self):\n",
    "        # Initialize the weights of the network given the sizes of the layers\n",
    "        self.weights = list()\n",
    "        for i in range(self.layer_sizes.shape[0]-1):\n",
    "            self.weights.append(np.random.uniform(-1,1,size=[self.layer_sizes[i],self.layer_sizes[i+1]]))\n",
    "        self.weights = np.asarray(self.weights)\n",
    "    \n",
    "    def __init_layers(self,batch_size):\n",
    "        # Initialize and allocate arrays for the hidden layer activations \n",
    "        self.__h = [np.empty((batch_size,layer)) for layer in self.layer_sizes]\n",
    "    \n",
    "    def __feed_forward(self,batch):\n",
    "        \"\"\"\n",
    "    Perform a forward pass of the neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch : array_like\n",
    "        (batch_size x dim) matrix of inputs\n",
    "    hidden_layers : list\n",
    "        List of hidden layer outputs\n",
    "    weights : array_like\n",
    "        Array of weight matricies\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output : array_like\n",
    "        Forward pass output of the MLP\n",
    "    hidden_layers : array_like\n",
    "        List of hidden layer outputs, populated from the forward pass.\n",
    "    \"\"\"\n",
    "        # Perform a forward pass of `batch` samples (N_samples x N_features)\n",
    "        h_l = batch\n",
    "        self.__h[0] = h_l\n",
    "        for i,weights in enumerate(self.weights):\n",
    "            h_l = self.__tanh(h_l.dot(weights))\n",
    "            self.__h[i+1]=h_l\n",
    "        self.__out = self.__softmax(self.__h[-1])\n",
    "    \n",
    "    def __back_prop(self,batch_y):\n",
    "        delta_t = (self.__out - batch_y)*self.__tanh_prime(self.__h[-1])\n",
    "        for i in range(1,len(self.weights)+1):\n",
    "            self.weights[-i]-=self.lr*(self.__h[-i-1].T.dot(delta_t))/self.batch_size\n",
    "            delta_t = self.__tanh_prime(self.__h[-i-1])*(delta_t.dot(self.weights[-i].T))\n",
    "        \"\"\"\n",
    "        Calculate derivative of sigmoid activation based on sigmoid output.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        output : array_like\n",
    "            Forward pass output of the MLP\n",
    "        batch_y : array_like\n",
    "            True labels for the samples in the batch\n",
    "        hidden_layers : list\n",
    "            List of hidden layer outputs  \n",
    "        weights : array_like\n",
    "            Array of weight matricies\n",
    "        lr : float\n",
    "            Learning rate for SGD\n",
    "        batch_size : int\n",
    "            Size of a training mini-batch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weights : array_like\n",
    "            Array of weight matricies, updated from the backpropagation.\n",
    "    \n",
    "        \"\"\"\n",
    "        # Update the weights of the network through back-propagation\n",
    "\n",
    "            \n",
    "    def predict(self,X):\n",
    "        # Generate a categorical, one-hot, prediction given an input X\n",
    "        X = np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n",
    "        self.__init_layers(X.shape[0])\n",
    "        self.__feed_forward(X)\n",
    "        return self.__to_categorical(self.__out)\n",
    "    \n",
    "    def evaluate(self,X,Y):\n",
    "        # Evaluate the performance (accuracy) predicting on X with true labels Y\n",
    "        prediction = self.predict(X)\n",
    "        return self.__accuracy(prediction,Y)\n",
    "        \n",
    "    def train(self,batch_size=8,epochs=25,lr=1.0):\n",
    "        # Train the model with a given batch size, epochs, and learning rate. Store and print relevant metrics.\n",
    "        self.lr = lr\n",
    "        self.batch_size=batch_size\n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "            \n",
    "            self.__init_layers(self.batch_size)\n",
    "            shuffle = np.random.permutation(self.n_samples)\n",
    "            train_loss = 0\n",
    "            train_acc = 0\n",
    "            X_batches = np.array_split(self.X[shuffle],self.n_samples/self.batch_size)\n",
    "            Y_batches = np.array_split(self.Y[shuffle],self.n_samples/self.batch_size)\n",
    "            for batch_x,batch_y in zip(X_batches,Y_batches):\n",
    "                self.__feed_forward(batch_x)  \n",
    "                train_loss += self.__loss(self.__out,batch_y)\n",
    "                train_acc += self.__accuracy(self.__to_categorical(self.__out),batch_y)\n",
    "                self.__back_prop(batch_y)\n",
    "                \n",
    "            train_loss = (train_loss/len(X_batches))\n",
    "            train_acc = (train_acc/len(X_batches))\n",
    "            self.train_loss.append(train_loss)\n",
    "            self.train_acc.append(train_acc)\n",
    "            \n",
    "            train_time = round(time.time()-start,3)\n",
    "            self.train_time.append(train_time)\n",
    "            \n",
    "            self.__init_layers(self.X_val.shape[0])\n",
    "            self.__feed_forward(self.X_val)\n",
    "            val_loss = self.__loss(self.__out,self.Y_val)\n",
    "            val_acc = self.__accuracy(self.__to_categorical(self.__out),self.Y_val)\n",
    "            self.val_loss.append(val_loss)\n",
    "            self.val_acc.append(val_acc)\n",
    "            \n",
    "            tot_time = round(time.time()-start,3)\n",
    "            self.tot_time.append(tot_time)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: loss = {train_loss.round(3)} | acc = {train_acc.round(3)} | val_loss = {val_loss.round(3)} | val_acc = {val_acc.round(3)} | train_time = {train_time} | tot_time = {tot_time}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's give this a try. Let's create a really simple MLP with only a single hidden layer `L=1` with 128 neurons `N_l=256`. We'll train with a `batch_size=50` for `epochs=100` and a learning rate `lr=0.8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soustab\\AppData\\Local\\Temp\\ipykernel_12828\\2763534016.py:72: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.weights = np.asarray(self.weights)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 3.335 | acc = 0.091 | val_loss = 3.099 | val_acc = 0.133 | train_time = 0.551 | tot_time = 0.564\n",
      "Epoch 2: loss = 2.856 | acc = 0.188 | val_loss = 2.747 | val_acc = 0.262 | train_time = 0.449 | tot_time = 0.463\n",
      "Epoch 3: loss = 2.528 | acc = 0.324 | val_loss = 2.562 | val_acc = 0.357 | train_time = 0.448 | tot_time = 0.461\n",
      "Epoch 4: loss = 2.34 | acc = 0.415 | val_loss = 2.483 | val_acc = 0.379 | train_time = 0.388 | tot_time = 0.401\n",
      "Epoch 5: loss = 2.223 | acc = 0.479 | val_loss = 2.446 | val_acc = 0.412 | train_time = 0.405 | tot_time = 0.419\n",
      "Epoch 6: loss = 2.149 | acc = 0.521 | val_loss = 2.42 | val_acc = 0.446 | train_time = 0.411 | tot_time = 0.424\n",
      "Epoch 7: loss = 2.098 | acc = 0.553 | val_loss = 2.397 | val_acc = 0.452 | train_time = 0.421 | tot_time = 0.435\n",
      "Epoch 8: loss = 2.054 | acc = 0.576 | val_loss = 2.392 | val_acc = 0.47 | train_time = 0.392 | tot_time = 0.405\n",
      "Epoch 9: loss = 2.023 | acc = 0.597 | val_loss = 2.388 | val_acc = 0.47 | train_time = 0.405 | tot_time = 0.419\n",
      "Epoch 10: loss = 1.993 | acc = 0.62 | val_loss = 2.39 | val_acc = 0.477 | train_time = 0.413 | tot_time = 0.429\n",
      "Epoch 11: loss = 1.973 | acc = 0.63 | val_loss = 2.405 | val_acc = 0.489 | train_time = 0.445 | tot_time = 0.461\n",
      "Epoch 12: loss = 1.955 | acc = 0.645 | val_loss = 2.399 | val_acc = 0.499 | train_time = 0.419 | tot_time = 0.432\n",
      "Epoch 13: loss = 1.941 | acc = 0.654 | val_loss = 2.404 | val_acc = 0.505 | train_time = 0.388 | tot_time = 0.4\n",
      "Epoch 14: loss = 1.931 | acc = 0.666 | val_loss = 2.42 | val_acc = 0.505 | train_time = 0.416 | tot_time = 0.43\n",
      "Epoch 15: loss = 1.923 | acc = 0.671 | val_loss = 2.418 | val_acc = 0.509 | train_time = 0.413 | tot_time = 0.428\n",
      "Epoch 16: loss = 1.915 | acc = 0.678 | val_loss = 2.421 | val_acc = 0.515 | train_time = 0.409 | tot_time = 0.422\n",
      "Epoch 17: loss = 1.908 | acc = 0.683 | val_loss = 2.423 | val_acc = 0.517 | train_time = 0.394 | tot_time = 0.406\n",
      "Epoch 18: loss = 1.902 | acc = 0.689 | val_loss = 2.42 | val_acc = 0.522 | train_time = 0.39 | tot_time = 0.403\n",
      "Epoch 19: loss = 1.898 | acc = 0.692 | val_loss = 2.423 | val_acc = 0.531 | train_time = 0.41 | tot_time = 0.423\n",
      "Epoch 20: loss = 1.894 | acc = 0.696 | val_loss = 2.42 | val_acc = 0.531 | train_time = 0.414 | tot_time = 0.427\n",
      "Epoch 21: loss = 1.89 | acc = 0.701 | val_loss = 2.425 | val_acc = 0.537 | train_time = 0.403 | tot_time = 0.414\n",
      "Epoch 22: loss = 1.887 | acc = 0.705 | val_loss = 2.425 | val_acc = 0.538 | train_time = 0.409 | tot_time = 0.426\n",
      "Epoch 23: loss = 1.884 | acc = 0.707 | val_loss = 2.423 | val_acc = 0.537 | train_time = 0.508 | tot_time = 0.522\n",
      "Epoch 24: loss = 1.881 | acc = 0.711 | val_loss = 2.422 | val_acc = 0.539 | train_time = 0.422 | tot_time = 0.435\n",
      "Epoch 25: loss = 1.879 | acc = 0.713 | val_loss = 2.423 | val_acc = 0.539 | train_time = 0.409 | tot_time = 0.424\n",
      "Epoch 26: loss = 1.877 | acc = 0.716 | val_loss = 2.424 | val_acc = 0.542 | train_time = 0.405 | tot_time = 0.419\n",
      "Epoch 27: loss = 1.875 | acc = 0.717 | val_loss = 2.427 | val_acc = 0.541 | train_time = 0.402 | tot_time = 0.415\n",
      "Epoch 28: loss = 1.873 | acc = 0.72 | val_loss = 2.426 | val_acc = 0.541 | train_time = 0.408 | tot_time = 0.423\n",
      "Epoch 29: loss = 1.872 | acc = 0.723 | val_loss = 2.424 | val_acc = 0.547 | train_time = 0.423 | tot_time = 0.436\n",
      "Epoch 30: loss = 1.87 | acc = 0.725 | val_loss = 2.424 | val_acc = 0.546 | train_time = 0.406 | tot_time = 0.42\n",
      "Epoch 31: loss = 1.869 | acc = 0.725 | val_loss = 2.423 | val_acc = 0.546 | train_time = 0.411 | tot_time = 0.424\n",
      "Epoch 32: loss = 1.867 | acc = 0.727 | val_loss = 2.425 | val_acc = 0.551 | train_time = 0.435 | tot_time = 0.449\n",
      "Epoch 33: loss = 1.866 | acc = 0.729 | val_loss = 2.424 | val_acc = 0.551 | train_time = 0.45 | tot_time = 0.465\n",
      "Epoch 34: loss = 1.865 | acc = 0.73 | val_loss = 2.423 | val_acc = 0.551 | train_time = 0.448 | tot_time = 0.462\n",
      "Epoch 35: loss = 1.864 | acc = 0.732 | val_loss = 2.426 | val_acc = 0.551 | train_time = 0.412 | tot_time = 0.426\n",
      "Epoch 36: loss = 1.863 | acc = 0.732 | val_loss = 2.423 | val_acc = 0.555 | train_time = 0.4 | tot_time = 0.413\n",
      "Epoch 37: loss = 1.861 | acc = 0.733 | val_loss = 2.424 | val_acc = 0.553 | train_time = 0.408 | tot_time = 0.421\n",
      "Epoch 38: loss = 1.86 | acc = 0.734 | val_loss = 2.425 | val_acc = 0.555 | train_time = 0.42 | tot_time = 0.433\n",
      "Epoch 39: loss = 1.859 | acc = 0.736 | val_loss = 2.425 | val_acc = 0.555 | train_time = 0.42 | tot_time = 0.436\n",
      "Epoch 40: loss = 1.859 | acc = 0.737 | val_loss = 2.424 | val_acc = 0.553 | train_time = 0.396 | tot_time = 0.409\n",
      "Epoch 41: loss = 1.858 | acc = 0.738 | val_loss = 2.423 | val_acc = 0.555 | train_time = 0.388 | tot_time = 0.402\n",
      "Epoch 42: loss = 1.857 | acc = 0.737 | val_loss = 2.424 | val_acc = 0.555 | train_time = 0.4 | tot_time = 0.413\n",
      "Epoch 43: loss = 1.856 | acc = 0.739 | val_loss = 2.423 | val_acc = 0.554 | train_time = 0.419 | tot_time = 0.433\n",
      "Epoch 44: loss = 1.856 | acc = 0.74 | val_loss = 2.42 | val_acc = 0.558 | train_time = 0.437 | tot_time = 0.455\n",
      "Epoch 45: loss = 1.856 | acc = 0.741 | val_loss = 2.424 | val_acc = 0.555 | train_time = 0.431 | tot_time = 0.445\n",
      "Epoch 46: loss = 1.855 | acc = 0.741 | val_loss = 2.42 | val_acc = 0.554 | train_time = 0.419 | tot_time = 0.433\n",
      "Epoch 47: loss = 1.854 | acc = 0.742 | val_loss = 2.42 | val_acc = 0.556 | train_time = 0.423 | tot_time = 0.438\n",
      "Epoch 48: loss = 1.853 | acc = 0.742 | val_loss = 2.42 | val_acc = 0.557 | train_time = 0.427 | tot_time = 0.44\n",
      "Epoch 49: loss = 1.853 | acc = 0.742 | val_loss = 2.416 | val_acc = 0.556 | train_time = 0.425 | tot_time = 0.44\n",
      "Epoch 50: loss = 1.852 | acc = 0.742 | val_loss = 2.417 | val_acc = 0.556 | train_time = 0.443 | tot_time = 0.458\n",
      "Epoch 51: loss = 1.851 | acc = 0.743 | val_loss = 2.417 | val_acc = 0.557 | train_time = 0.418 | tot_time = 0.431\n",
      "Epoch 52: loss = 1.851 | acc = 0.744 | val_loss = 2.418 | val_acc = 0.557 | train_time = 0.41 | tot_time = 0.425\n",
      "Epoch 53: loss = 1.851 | acc = 0.745 | val_loss = 2.415 | val_acc = 0.557 | train_time = 0.387 | tot_time = 0.4\n",
      "Epoch 54: loss = 1.85 | acc = 0.745 | val_loss = 2.418 | val_acc = 0.555 | train_time = 0.39 | tot_time = 0.403\n",
      "Epoch 55: loss = 1.85 | acc = 0.745 | val_loss = 2.418 | val_acc = 0.559 | train_time = 0.409 | tot_time = 0.423\n",
      "Epoch 56: loss = 1.85 | acc = 0.746 | val_loss = 2.417 | val_acc = 0.559 | train_time = 0.411 | tot_time = 0.424\n",
      "Epoch 57: loss = 1.849 | acc = 0.746 | val_loss = 2.417 | val_acc = 0.56 | train_time = 0.399 | tot_time = 0.412\n",
      "Epoch 58: loss = 1.849 | acc = 0.747 | val_loss = 2.417 | val_acc = 0.558 | train_time = 0.39 | tot_time = 0.403\n",
      "Epoch 59: loss = 1.849 | acc = 0.748 | val_loss = 2.416 | val_acc = 0.559 | train_time = 0.395 | tot_time = 0.41\n",
      "Epoch 60: loss = 1.849 | acc = 0.748 | val_loss = 2.415 | val_acc = 0.56 | train_time = 0.422 | tot_time = 0.434\n",
      "Epoch 61: loss = 1.848 | acc = 0.75 | val_loss = 2.416 | val_acc = 0.561 | train_time = 0.411 | tot_time = 0.425\n",
      "Epoch 62: loss = 1.848 | acc = 0.75 | val_loss = 2.413 | val_acc = 0.561 | train_time = 0.397 | tot_time = 0.41\n",
      "Epoch 63: loss = 1.847 | acc = 0.751 | val_loss = 2.413 | val_acc = 0.561 | train_time = 0.392 | tot_time = 0.404\n",
      "Epoch 64: loss = 1.847 | acc = 0.751 | val_loss = 2.414 | val_acc = 0.56 | train_time = 0.403 | tot_time = 0.417\n",
      "Epoch 65: loss = 1.847 | acc = 0.752 | val_loss = 2.413 | val_acc = 0.563 | train_time = 0.409 | tot_time = 0.423\n",
      "Epoch 66: loss = 1.846 | acc = 0.753 | val_loss = 2.413 | val_acc = 0.561 | train_time = 0.474 | tot_time = 0.488\n",
      "Epoch 67: loss = 1.846 | acc = 0.753 | val_loss = 2.414 | val_acc = 0.564 | train_time = 0.404 | tot_time = 0.418\n",
      "Epoch 68: loss = 1.845 | acc = 0.754 | val_loss = 2.415 | val_acc = 0.563 | train_time = 0.407 | tot_time = 0.423\n",
      "Epoch 69: loss = 1.845 | acc = 0.754 | val_loss = 2.413 | val_acc = 0.561 | train_time = 0.42 | tot_time = 0.434\n",
      "Epoch 70: loss = 1.844 | acc = 0.755 | val_loss = 2.412 | val_acc = 0.561 | train_time = 0.437 | tot_time = 0.452\n",
      "Epoch 71: loss = 1.844 | acc = 0.756 | val_loss = 2.413 | val_acc = 0.563 | train_time = 0.426 | tot_time = 0.441\n",
      "Epoch 72: loss = 1.844 | acc = 0.756 | val_loss = 2.412 | val_acc = 0.565 | train_time = 0.426 | tot_time = 0.44\n",
      "Epoch 73: loss = 1.844 | acc = 0.756 | val_loss = 2.414 | val_acc = 0.562 | train_time = 0.412 | tot_time = 0.426\n",
      "Epoch 74: loss = 1.844 | acc = 0.757 | val_loss = 2.412 | val_acc = 0.565 | train_time = 0.411 | tot_time = 0.426\n",
      "Epoch 75: loss = 1.843 | acc = 0.757 | val_loss = 2.412 | val_acc = 0.564 | train_time = 0.409 | tot_time = 0.422\n",
      "Epoch 76: loss = 1.843 | acc = 0.757 | val_loss = 2.412 | val_acc = 0.565 | train_time = 0.399 | tot_time = 0.413\n",
      "Epoch 77: loss = 1.843 | acc = 0.757 | val_loss = 2.412 | val_acc = 0.565 | train_time = 0.398 | tot_time = 0.411\n",
      "Epoch 78: loss = 1.842 | acc = 0.758 | val_loss = 2.412 | val_acc = 0.568 | train_time = 0.41 | tot_time = 0.423\n",
      "Epoch 79: loss = 1.842 | acc = 0.758 | val_loss = 2.413 | val_acc = 0.565 | train_time = 0.413 | tot_time = 0.427\n",
      "Epoch 80: loss = 1.842 | acc = 0.758 | val_loss = 2.41 | val_acc = 0.566 | train_time = 0.409 | tot_time = 0.424\n",
      "Epoch 81: loss = 1.841 | acc = 0.758 | val_loss = 2.413 | val_acc = 0.563 | train_time = 0.394 | tot_time = 0.408\n",
      "Epoch 82: loss = 1.841 | acc = 0.759 | val_loss = 2.412 | val_acc = 0.563 | train_time = 0.401 | tot_time = 0.415\n",
      "Epoch 83: loss = 1.841 | acc = 0.759 | val_loss = 2.412 | val_acc = 0.563 | train_time = 0.41 | tot_time = 0.422\n",
      "Epoch 84: loss = 1.84 | acc = 0.759 | val_loss = 2.411 | val_acc = 0.564 | train_time = 0.43 | tot_time = 0.445\n",
      "Epoch 85: loss = 1.84 | acc = 0.759 | val_loss = 2.411 | val_acc = 0.564 | train_time = 0.436 | tot_time = 0.452\n",
      "Epoch 86: loss = 1.84 | acc = 0.76 | val_loss = 2.41 | val_acc = 0.562 | train_time = 0.426 | tot_time = 0.441\n",
      "Epoch 87: loss = 1.84 | acc = 0.76 | val_loss = 2.411 | val_acc = 0.563 | train_time = 0.446 | tot_time = 0.461\n",
      "Epoch 88: loss = 1.839 | acc = 0.76 | val_loss = 2.41 | val_acc = 0.563 | train_time = 0.453 | tot_time = 0.471\n",
      "Epoch 89: loss = 1.839 | acc = 0.76 | val_loss = 2.411 | val_acc = 0.563 | train_time = 0.449 | tot_time = 0.464\n",
      "Epoch 90: loss = 1.839 | acc = 0.76 | val_loss = 2.41 | val_acc = 0.565 | train_time = 0.432 | tot_time = 0.445\n",
      "Epoch 91: loss = 1.839 | acc = 0.76 | val_loss = 2.411 | val_acc = 0.565 | train_time = 0.4 | tot_time = 0.414\n",
      "Epoch 92: loss = 1.839 | acc = 0.76 | val_loss = 2.411 | val_acc = 0.565 | train_time = 0.404 | tot_time = 0.418\n",
      "Epoch 93: loss = 1.839 | acc = 0.76 | val_loss = 2.411 | val_acc = 0.565 | train_time = 0.406 | tot_time = 0.419\n",
      "Epoch 94: loss = 1.839 | acc = 0.76 | val_loss = 2.411 | val_acc = 0.566 | train_time = 0.405 | tot_time = 0.418\n",
      "Epoch 95: loss = 1.838 | acc = 0.761 | val_loss = 2.41 | val_acc = 0.567 | train_time = 0.388 | tot_time = 0.403\n",
      "Epoch 96: loss = 1.838 | acc = 0.761 | val_loss = 2.412 | val_acc = 0.567 | train_time = 0.393 | tot_time = 0.406\n",
      "Epoch 97: loss = 1.838 | acc = 0.762 | val_loss = 2.413 | val_acc = 0.567 | train_time = 0.415 | tot_time = 0.429\n",
      "Epoch 98: loss = 1.838 | acc = 0.762 | val_loss = 2.412 | val_acc = 0.567 | train_time = 0.413 | tot_time = 0.427\n",
      "Epoch 99: loss = 1.838 | acc = 0.762 | val_loss = 2.412 | val_acc = 0.568 | train_time = 0.399 | tot_time = 0.413\n",
      "Epoch 100: loss = 1.838 | acc = 0.762 | val_loss = 2.412 | val_acc = 0.567 | train_time = 0.391 | tot_time = 0.403\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X_train,Y_train,X_test,Y_test,L=1,N_l=256)\n",
    "model.train(batch_size=50,epochs=100,lr=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "ax[0].plot(model.train_loss,label=\"Train loss\")\n",
    "ax[0].plot(model.val_loss,label=\"Val loss\")\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].plot(model.train_acc,label=\"Train acc\")\n",
    "ax[1].plot(model.val_acc,label=\"Val acc\")\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5694070080862533"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score, Recall, and Precision:\n",
    "\n",
    "Precision is the ratio of true positives to the total number of predicted positives. It measures how many of the predicted positive cases are actually positive. A high precision score indicates that the model is good at predicting positive cases.\n",
    "\n",
    "Recall is the ratio of true positives to the total number of actual positives. It measures how many of the actual positive cases are correctly predicted by the model. A high recall score indicates that the model is good at identifying positive cases.\n",
    "\n",
    "F1 Score is the harmonic mean of precision and recall. It is a single score that balances both precision and recall. A high F1 score indicates that the model is good at both identifying positive cases and avoiding false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.5587620254107325\n",
      "Recall: 0.5694070080862533\n",
      "Precision: 0.5543389396341125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred=(np.argmax(y_pred, axis=1)+1).reshape(-1, 1).flatten()\n",
    "y_test_c=(np.argmax(y_test, axis=1)+1).reshape(-1, 1).flatten()\n",
    "# print(y_test_c)\n",
    "print ('F1 score:', f1_score(y_test_c, y_pred,average='weighted'))\n",
    "print ('Recall:', recall_score(y_test_c, y_pred,average='weighted'))\n",
    "print ('Precision:', precision_score(y_test_c, y_pred,average='weighted'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
