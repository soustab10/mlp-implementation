{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron for Optical Character Recognition\n",
    "\n",
    "Amitesh Sahu 21074006 \n",
    "\n",
    "Rituraj Barai 21074024\n",
    "\n",
    "Soustab Haldar 21074029\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:16:15.682784Z",
     "start_time": "2019-07-15T22:16:15.559468Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "One of the first steps when working with a new data set is preprocessing. However you come about your data, whether downloaded from an established dataset or created yourself, it's vital to carefully examine and prepare your data. Our data here is from the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) of `28x28` pixel images of handwritten digits from `0-9`, which has an important place in the history of machine learning. Some common questions to ask yourself when confronted with a new data set when preparing analysis:\n",
    "1. What range of values does your data take? Do values need to be rescaled?\n",
    "2. What shape is your data? Should each sample be a matrix ($n \\times m$), a vector ($1 \\times n$), or even a multi-dimensional tensor ($n \\times m \\times p \\times \\ldots$)?\n",
    "3. Are there fringe cases that need special attention?\n",
    "\n",
    "Our data lives in the directory `data` under the file names `mnist_train.csv` and `mnist_test.csv`, so the complete file path (from the main directory) are `data/mnist_train.csv` and `data/mnist_test.csv`. With that in mind, let's define a helper function `load_data` that we can use to quickly in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "e\n",
      "f\n",
      "g\n",
      "h\n",
      "i\n",
      "j\n",
      "k\n",
      "l\n",
      "m\n",
      "n\n",
      "o\n",
      "p\n",
      "q\n",
      "r\n",
      "s\n",
      "t\n",
      "u\n",
      "v\n",
      "w\n",
      "x\n",
      "y\n",
      "z\n"
     ]
    }
   ],
   "source": [
    "root_dir=r\"D:\\IIT BHU\\SEM V\\IC\\mlp-implementation\\imgs\\\\\"\n",
    "dict = {chr(i): i - ord('a') + 1 for i in range(ord('a'), ord('z') + 1)}\n",
    "lis = []\n",
    "for i in os.listdir(root_dir):\n",
    "    path=os.path.join(root_dir,i)\n",
    "    for j in os.listdir(path):\n",
    "        img_path=os.path.join(path,j)\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        l=[dict[i]]\n",
    "        li=list(img.getdata())\n",
    "        l=l+li\n",
    "        lis+=[l]\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir=r\"D:\\IIT BHU\\SEM V\\IC\\mlp-implementation\\data\\data.csv\"\n",
    "with open(csv_dir, 'w',newline='') as csvfile:   \n",
    "    csvwriter = csv.writer(csvfile)  \n",
    "    csvwriter.writerows(lis) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:16:16.437154Z",
     "start_time": "2019-07-15T22:16:16.431149Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_data(dir_name):\n",
    "    \"\"\"\n",
    "    Function for loading MNIST data stored in comma delimited files. Labels for \n",
    "    each image are the first entry in each row.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dit_name : str\n",
    "         Path to where data is contained\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : array_like\n",
    "        A (N x p=784) matrix of samples \n",
    "    Y : array_like\n",
    "        A (N x 1) matrix of labels for each sample\n",
    "    \"\"\"\n",
    "    data = list() # init a list called `data`\n",
    "    \n",
    "    with open(dir_name,\"r\") as f: # open the directory as a read (\"r\"), call it `f`\n",
    "        for line in f: # iterate through each `line` in `f`\n",
    "            split_line = np.array(line.split(',')) # split lines by `,` - cast the resultant list into an numpy array\n",
    "            split_line = split_line.astype(np.float32) # make the numpy array of str into floats\n",
    "            data.append(split_line) # collect the sample into the `data` list\n",
    "            \n",
    "    data = np.asarray(data) # convert the `data` list into a numpy array for easier indexing\n",
    "    \n",
    "    # as the first number in each sample is the label (0-9), extract that from the rest and return both (X,Y)\n",
    "    return data[:,1:],data[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:10:03.705155Z",
     "start_time": "2019-07-15T20:09:44.341375Z"
    }
   },
   "outputs": [],
   "source": [
    "X_data,Y_data = load_data(\"data/data.csv\")\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X_data,Y_data,test_size=0.1,random_state=104)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `.shape` attribute of numpy arrays. We see that the shape of `X_train` is `6675 x 784`, thus there are `6675` samples (images) each with dimension `784`. Each sample, typically presented as a 28 x 28 image, is unrolled into a 1-dimensional vector 28 x 28 = 784 contained within each row of `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:10:03.720141Z",
     "start_time": "2019-07-15T20:10:03.717400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training set is: 6675 x 784\n",
      "The shape of the test set is: 742 x 784\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of the training set is: {X_train.shape[0]} x {X_train.shape[1]}\")\n",
    "print(f\"The shape of the test set is: {X_test.shape[0]} x {X_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples are represented, we can do this by calling `Y_train[index]` and `X_train[index]` (here I choose `index=0` to look at the very first sample). We first notice `Y_train[0]=5.0`, meaning this entry is the digit `5`. We will confirm this shortly by visualizing some of these samples. We then notice each entry is an integer (cast into `np.float32` in our `load_data` function) ranging from `0-255`. This representation is common when working with images. The numerical entries are interpreted as pixel intensities typically shown in gray-scale ranging between `0` (black) and `255` (white). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:10:03.766216Z",
     "start_time": "2019-07-15T20:10:03.734027Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19.0,\n",
       " array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          3.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,  73., 114., 125., 128., 128.,\n",
       "        128., 100.,  41.,   0.,  10.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  58., 155.,\n",
       "        230., 233., 177., 130., 118., 128.,  89., 145., 204., 203.,  40.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,  12., 165., 245., 143., 103.,  86., 101.,  88.,  70.,\n",
       "         71., 104., 106.,   1.,  36.,   3.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 149., 160.,  64.,\n",
       "        118.,  74.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0., 152., 233.,  66.,  86.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  25., 229.,\n",
       "        197.,  38.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,  21., 143., 236., 210., 170., 144.,\n",
       "        128., 128., 128., 151., 170., 170., 134.,  96.,  18.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,  27., 114., 114., 121., 128., 128., 128., 128., 128.,\n",
       "        128., 144., 211., 244., 103.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  63.,\n",
       "         92.,  75.,   0.,   0.,   0.,   0.,  89.,   7.,   0.,  85., 136.,\n",
       "        134.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,  64.,  70., 163., 154.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  69., 103.,\n",
       "        117., 154.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,  24., 102., 199., 225., 141.,  10.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   5.,  30.,  63.,  90., 137., 174., 230., 255., 212., 129.,\n",
       "         45.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0., 136., 170., 170., 155.,\n",
       "        128.,  95.,  58.,  21.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "Y_train[index], X_train[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is nothing in principle wrong with this `0-255` representation, the value of `255` being the maximum is specific to images and somewhat an arbitrary for our purposes. Frequently people opt to rescale their data to range between `0-1`, which will be have some nice mathematical properties for us later. Of course, this can be done by simply dividing each entry in `X_train` and `X_test` by its maximum value (accessed using `X_train.max()`). \n",
    "\n",
    "Further, each label in `Y_train` and `Y_test` are currently integers (e.g. `5.0` or `2.0`). For categorical data (where image is labeled between `0-9`) we opt for a one-hot encoded representation of our data. What this means is each label is converted into a binary vector (e.g. the label 2.0 would be converted to the vector `[0,0,1,0,0,0,0,0,0,0]` and the label 9.0 would be the vector `[0,0,0,0,0,0,0,0,0,1]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:10:03.883943Z",
     "start_time": "2019-07-15T20:10:03.780726Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soustab\\AppData\\Local\\Temp\\ipykernel_4100\\3197139449.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_train[np.arange(Y_train.size),Y_train.astype(np.int)] = 1.0\n",
      "C:\\Users\\Soustab\\AppData\\Local\\Temp\\ipykernel_4100\\3197139449.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_test[np.arange(Y_test.size),Y_test.astype(np.int)] = 1.0\n"
     ]
    }
   ],
   "source": [
    "# rescale data between 0 - 1.0\n",
    "X_train = X_train/X_train.max()\n",
    "X_test = X_test/X_test.max()\n",
    "\n",
    "# one-hot encode train (y_train) and test (y_test) set labels\n",
    "y_train = np.zeros((Y_train.size, int(Y_train.max()) + 1))\n",
    "y_train[np.arange(Y_train.size),Y_train.astype(np.int)] = 1.0\n",
    "\n",
    "y_test = np.zeros((Y_test.size, int(Y_test.max()) + 1))\n",
    "y_test[np.arange(Y_test.size),Y_test.astype(np.int)] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed, let's go ahead and visualize some of the inputs and their associated labels. We can do this using the `imshow` function (making sure to resize the flattened size 784 representation into a 28 x 28 matrix for compatability with `imshow`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:10:04.313194Z",
     "start_time": "2019-07-15T20:10:03.897501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_images = 4\n",
    "fig,axes = plt.subplots(1,num_images,figsize=(15,10))\n",
    "for image,label,ax in zip(X_train[:num_images],y_train[:num_images],axes):\n",
    "    ax.imshow(image.reshape(28,28),cmap='gray',vmin=0,vmax=1.0)\n",
    "    for i in range(len(label)):\n",
    "        if(label[i]==1.0):\n",
    "            lab=list(dict.keys())[list(dict.values()).index(i)]\n",
    "    ax.set_title(f\"Label: {lab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer perceptron: the math \n",
    "Here some of the math underlying the multilayer perceptron (MLP) and the backpropagation algorithm are reviewed. The MLP and backpropagation are central to understanding deep learning as a whole. Full stop. What I'm presenting here is by any means *not* an exhaustive exposition of the subject, and I **highly** recommend reading (at least) [chapters 1 & 2 of the free online book: Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) for a more complete discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is a multilayer perceptron? Before we answer that question, let's dissect just a singular component: the perceptron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:44:16.088920Z",
     "start_time": "2019-07-15T20:44:15.924981Z"
    }
   },
   "source": [
    "![Perceptron](imgs/perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image credit: https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image above represents schematically the conventional perceptron. Each input, represented as a multidimensional vector $\\mathbf{x}=\\{x_0=1,x_1,x_2,\\ldots,x_n\\}$, is multiplied by a set of weights $\\mathbf{w} = \\{w_0,w_1,w_2,\\ldots,w_n\\}$ to produce the weighted sum $$z = \\sum_{i=0}^{n} x_i w_i$$. In the case of a vanilla perceptron, this weighted sum $z$ is lastly fed through as step function to produce the predicted output $a = \\text{step}(z)$. This results in the output where $a=1$ if $z>0$ and $a=0$ if $z<0$.\n",
    "\n",
    "With the output $a$ taking the value 0 or 1, this type of structure really only works for binary classification problems (i.e. situations where each input $\\mathbf{x}$ has one of two intended labels: 0 or 1). The weights $\\mathbf{w}$ in this context can be interpreted as assigning credence to the different inputs $x_i$ based on their relative importance. Notice, there is an additional constant $x_0=1$ concatenated to each input, frequently called the *bias term* or just *the bias*. As there is still an associated weight $w_0$ along with this term, this effectively acts as an offset to the origin for the classification. In other words, think of the perceptron classification boundary as a multidimensional line (or plane) $y = m * x + b$. Inputs on one side of the line where $y<0$ get classified as one label ($a=0$), while  inputs on the other side of the line where $y>0$ are classified as the other label ($a=1$). In this geometric interpretation of the perceptron, the weights are like the slope (or vector perpendicular to the plane in higher dimensions) $\\mathbf{w} \\sim m$, and the bias is like the $b$ term $b \\sim w_0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending the perceptron to the multilayer perceptron (MLP) simply introduces a intermediate preceptrons (layers) before reading the output and generalizes the step function to a broader class of functions called *activation functions*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLP](imgs/MLP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image credit: https://github.com/PetarV-/TikZ/tree/master/Multilayer%20perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These neural networks are often schematically represented with a series of nodes and arrows as shown on the right. Each node represents one input, for example the input layer here is shown with 3 inputs, and each arrow represents a *unique* weight given to that particular input in the weighted sum (right side of figure). Note, in our application to MNIST each input would be `784+1` dimensional (including the bias) and the output `10` dimensional for representing the one-hot encoded binary vector of output labels. Rather than restricting ourselves to treating each sum with a step function as in the vanilla perceptron, this concept is generalized to any arbitrary function typically called the *activation function*, depicted as $\\sigma(\\cdot)$ in the schematic. In principle almost any function may be an activation function, however people typically use one of the following activation functions due to some nice mathematical properties we will discuss later:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![activation_func](imgs/activation_func.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:29:56.750472Z",
     "start_time": "2019-07-15T21:29:56.746122Z"
    }
   },
   "source": [
    "Image credit: https://medium.com/@shrutijadon10104776/survey-on-activation-functions-for-deep-learning-9689331ba092"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additional layers between the inputs and outputs are typically called *hidden layers* because the particular value of the nodes within these layers are typically unknown, and largely uninterpretable, to the user. A natural question to ask here is why are hidden layers helpful/necessary for deep learning? The answer is, shockingly, that they aren't necessary, but are very helpful. There exists a theorem called the \"Universal Approximation Theorem\" which states basically that a neural network with a single layer can approximate *any* non-linear function to arbitrary accuracy. While this may imply that only a single hidden layer is, in principle, sufficient for any problem, the necessary dimension of this hidden layer may become intractably large for some problems. Introducing many hidden layers allows your neural network to learn a hierarchy of concepts in the structure of the data. What this implies for the MLP is that earlier layers (i.e. the layers closer to the input) learn a coarser representation of the data and begins to more closely reflect the output representation nearer to the output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![deep](imgs/deep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:15:42.928623Z",
     "start_time": "2019-07-15T22:15:42.799116Z"
    }
   },
   "source": [
    "Image credit: UChicago STAT 37710 Spring 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed forward\n",
    "Given a predefined neural network *architecture* (the *architecture* of a neural network refers to all the elements necessary to completely define the flow of data, which involve the number and size of hidden layers, which activation functions, the output size, etc.) the process of generating an output from an input is called a *forward* pass. As we shall see, for an MLP the forward pass may be succinctly represented as a series of matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:16:19.367434Z",
     "start_time": "2019-07-15T22:16:19.254509Z"
    }
   },
   "source": [
    "![MLP_big](imgs/MLP_big.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:16:35.149302Z",
     "start_time": "2019-07-15T22:16:35.038196Z"
    }
   },
   "source": [
    "Image credit: http://neuralnetworksanddeeplearning.com/chap1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the MLP represented schematically above with sigmoid activations $\\sigma$ in the hidden layer. Each neuron in the hidden layers will be weighted sums of the inputs: $$a_j = \\sigma(\\sum_{i=1}^{784} x_i* w_{i,j})$$ for $j=\\{1,2,\\ldots,15\\}$ (notice, there is no bias term in this example). From here it clear to see that the weights $w_{i,j}$ may be compacted into a matrix $W \\in {\\rm I\\!R}^{784\\times15}$ where $W_{i,j} = w_{i,j}$, allowing for all the neurons in the hidden layer to be efficiently calculated using matrix multiplication: $\\mathbf{a}^{(1)} = \\sigma(\\mathbf{z}^{(1)}) = \\sigma(\\mathbf{x}W)$. Where $\\mathbf{a}^{(i)},\\mathbf{z}^{(i)} \\in {\\rm I\\!R}^{1\\times 15}$, with the superscript $(i)$ indicating the assocaited layer number, and $\\mathbf{x} \\in {\\rm I\\!R}^{1\\times 784}$ are both arranged as column vectors. Another, different, weight matrix is needed to transform the hidden layer to the output layer. Let's demarcate these two as $W^{(1)}$ for the matrix which transforms the inputs to the hidden layer $\\mathbf{a}^{(1)}$ and $W^{(2)} \\in {\\rm I\\!R}^{15\\times 10}$ for transforming the hidden layer to the output layer $\\mathbf{z}^{(2)}$. Mathematically, $$\\mathbf{z}^{(2)} = \\sigma(\\mathbf{x}W^{(1)})W^{(2)}$$. Notice, we've yet to treat this output $\\mathbf{z}^{(2)}$ with an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:32:36.274844Z",
     "start_time": "2019-07-15T22:32:36.261999Z"
    }
   },
   "source": [
    "The last step of process for classification tasks is actually producing a prediction from these numbers in the output layer $\\mathbf{z}^{(2)}$. Typically, in the case of multi-label classification, this is done using a *softmax* activation function which effectively converts the output neurons into probabilities for each label. This has the mathematical form, $$\\text{softmax}(\\mathbf{z})_j = \\frac{\\exp(z_j)}{\\sum_{k=0}^{K=9}\\exp{(z_k)}}$$.\n",
    "\n",
    "The softmax activation function has the property of the outputs summing to 1 $\\sum_{k=1}^{K=9}\\text{softmax}(\\mathbf{z}^{(2)})_k = 1$, allowing each output $\\mathbf{a}^{(2)}_i = \\text{softmax}(\\mathbf{z}^{(2)})_i$ to be interpreted as the probability that the input is actually a digit `0-9`. Note, the output of a MLP does not need to have a softmax activation, for example in a regression setting a softmax activation would not make much sense. When evaluating the classification accuracy of the neural network, the input is typically classified according to the output label with the highest probability, $$\\text{prediction}(\\mathbf{x}) = \\text{argmax}\\  \\text{softmax}(\\mathbf{z}^{(2)}) = \\text{argmax}\\  \\text{softmax}((\\sigma(\\mathbf{x}W^{(1)})W^{(2)}))$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "Now that we have an understanding of how an MLP generates outputs from inputs, we engage the problem of how to actually *train* this neural network. *Training* a neural network (in a supervised setting, which means each input comes with a known output) refers to the process of iteratively updating the weights of the network to improve it's performance. The performance of the neural network is evaluated using a *loss function* which quantitatively measures how \"close\" the neural network output is to the true output. In short, using *backpropagation* we aim to minimize the loss function with respect to the weights (also called *parameters*) of the neural network.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define precisely what we mean by a *cost function*. The cost function $C$ should reflect whatever objective your aim is for the neural network. Here, we're interested in having our neural network predict a digit label `0-9` for each input image. A natural, and very reasonable, choice in many settings is simply using the L2 norm between our predicted and true outputs $$C = \\frac{1}{2N}\\sum_{i=1}^N||y_i - \\mathbf{a}^{(L)}(\\mathbf{x}_i)||^2$$, where $y_i$ represents the true label corresponding to input $\\mathbf{x}_i$ and $\\mathbf{a}^{(L)}(\\mathbf{x}_i)$ is the output of the neural network with $L$ layers taking $\\mathbf{x}_i$ as input. Note, in our previous example $\\mathbf{a}^{(L=2)}(\\mathbf{x}_i) = \\text{softmax}((\\sigma(\\mathbf{x}_iW^{(1)})W^{(2)}))$. Of course, when minimizing/maximizing a function any constant multiplies have no effect on the optimum which allows us to include the $\\frac{1}{2N}$ as a mathematical convenience whose purpose will be clear shortly (spoiler, it has to do with derivatives). We may equivalently write this cost function in terms of the cost incurred by each sample individually as $$C = \\frac{1}{N}\\sum_{i=1}^N C_i$$. Where the cost of an individual training sample is of course, in the case of our quadratic cost function, $C_i = \\frac{1}{2}||y_i - \\mathbf{a}^{(L)}(\\mathbf{x}_i)||^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, training our neural network by incrementally adjusting the weights amounts to updating each weight $W^{(l)}_{i,j}$ based on its individual influence to the cost function. This can be done with the *gradient descent* algorithm that defines the update rule $$W^{(l)}_{i,j} \\leftarrow W^{(l)}_{i,j} - \\alpha \\frac{\\partial C}{\\partial W^{(l)}_{i,j}}$$, where $\\alpha$ is a tunable parameter called the *learning rate*. Notice, vanilla gradient descent requires computing the derivative (people in the ML community use the word gradient, rather than derivative) $\\frac{\\partial C}{\\partial W^{(l)}_{i,j}}$ with respect to **all** the training samples. Of course, this can become expensive very quickly as some datasets, for example, contain millions of image and it's simply intractable for computational reasons to work with all that data at once. Therefore, people typically (read, always) employ a modified version of gradient descent called *stochastic gradient descent* (SGD) which updates the weights in *batches* (also called *mini batches*) of size $m$ $$W^{(l)}_{i,j} \\leftarrow W^{(l)}_{i,j} - \\frac{\\alpha}{m}\\sum_{k \\in \\text{batch}} \\frac{\\partial C_k}{\\partial W^{(l)}_{i,j}}$$. Where $C_i$ is the previously discussed cost function for an individual training sample $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to actually perform SGD we are left with the task of evaluating the gradient $\\frac{\\partial C_i}{\\partial W^{(l)}_{i,j}}$. However, we must be able to evaluate this gradient quickly and ideally in parallel as this calculation will be performed many, many times. This, finally, is where the *backpropagation* algorithm comes into play. \n",
    "\n",
    "Let's define the quantity $$\\delta^{(l)}_j = \\frac{\\partial C}{\\partial z^{(l)}_j}$$.\n",
    "\n",
    "Recalling that each hidden layer is expressed as $a^{(l)}_i = \\sigma(z^{(l)}_i) = \\sigma(\\sum_j W^{(l)}_{i,j} * a^{(l-1)}_j)$. Note, for simplicity I've omitted the bias term from $z$, and am using $\\partial C$ rather than $\\partial C_i$ for notational convenience. This $\\delta^{(l)}_j$ term may be thought of as the output error for neuron $j$ in layer $l$ with respect to the cost function $C$. Using $\\delta^{(l)}_j$ we can get at our sought after gradient by applying the chain rule $$\\frac{\\partial C_i}{\\partial W^{(l)}_{i,j}} = \\frac{\\partial C}{\\partial z^{(l)}_i} \\frac{\\partial z^{(l)}_i}{\\partial W^{(l)}_{i,j}} = \\delta^{(l)}_i * a^{(l-1)}_j$$. Where $\\frac{\\partial z^{(l)}_j}{\\partial W^{(l)}_{i,j}} = a^{(l-1)}$ can be readily derived recalling that $z^{(l)}_i = \\sum_k W^{(l)}_{i,k} * a^{(l-1)}_k$.\n",
    "\n",
    "Excellent! Now we have a closed-form analytical expression for $\\frac{\\partial C_i}{\\partial W_{i,j}}$. But how do we actually compute $\\delta^{(l)}_j$ in this expression? Let's start with the last layer $L$ and apply the chain rule again, $$\\delta^{(L)}_j = \\frac{\\partial C}{\\partial a^{(L)}_j} \\frac{\\partial a^{(L)}_j}{\\partial z^{(L)}_j} = \\frac{\\partial C}{\\partial a^{(L)}_j} \\sigma^{\\prime}(z^{(L)}_j)$$. The first term $\\frac{\\partial C}{\\partial a^{(L)}_j}$ is simply the error in the cost function with respect to the last layer, a quantity which can frequently be directly derived. For instance, in the case of the quadratic cost function $C = \\frac{1}{2}||y - a^{(L)}_{j}||^2$, the derivative is simply $\\frac{\\partial C}{\\partial a^{(L)}_j} = (a^{(L)}_j - y)$. Next, the $\\sigma^{\\prime}(z^{(L)}_j)$ is nothing more than the derivative of the activation function at numerical value of $z^{(L)}_j$. This is where the previously mentioned mathematical convenience of select activation functions comes in, say, for the sigmoid activation: $\\sigma^{\\prime}(x) = \\sigma(x) (1-\\sigma(x))$. What this means is that once we have computed $\\sigma(z^{(l)}_j)$ (for any $l$) during the forward pass of the network, we can store and reuse that value to efficiently compute $\\sigma^{\\prime}(z^{(l)}_j)$ during backpropagation!\n",
    "\n",
    "Okay, this is the last step. With the error in the last layer $\\delta^{(L)}_j$ computed the final piece of the puzzle is computing the error for the remaining layers $\\delta^{(l)}_j$. Here, I will simply state the formula for the sake of brevity -- but I encourage you to look at the proof available [here](http://neuralnetworksanddeeplearning.com/chap2.html), or even better, try to prove it yourself! \n",
    "\n",
    "$$\\boldsymbol{\\delta}^{l}=((W^{(l+1)})^{T} \\boldsymbol{\\delta}^{l+1}) \\odot \\sigma^{\\prime}(\\mathbf{z}^{l})$$ \n",
    "\n",
    "The only new notation introduced here is the *hadamard product* $\\odot$: which simply performs an element-wise multiplication along two vectors (e.g. $(\\mathbf{x} \\odot \\mathbf{y})_i = x_i * y_i$). Looking at this equation, we realize now why backpropagation is called *backpropagation*: the errors for a given layer $\\boldsymbol{\\delta}^{l}$ depend on the errors in the following layer $\\boldsymbol{\\delta}^{l+1}$. Thus, once we have computed the error in the last layer $\\boldsymbol{\\delta}^{L}$, backpropagation effectively works backward to compute the errors in the remaining layers which are then purposed to calcaulte the gradients $\\frac{\\partial C_i}{\\partial W^{(l)}_{i,j}}$ used to update the weights $W^{(l)}_{i,j}$ in SGD. That's it. That's the math which drives how neural networks *actually* learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer perceptron: the code\n",
    "Now that we have a handle on the math behind the MLP, we can now go ahead and implement everything we've looked at in code. Of course, we want to write efficient and fast code, which when writing in Python mean using matrix (NumPy multidimensional arrays) operations whenever possible. The mindset of using NumPy arrays is generally good practice when trying to write efficient code in Python, as NumPy will automatically employ thread parallelism when performing many operations on NumPy arrays which will help substantially to accelerate our code (this is also sometimes called *vectorizing* your code).\n",
    "\n",
    "We'll first go through and write the code for each piece of an MLP in generic Python functions. We'll then wrap everything in an `MLP` class, which will allow us to easily access all the MLP functionality in a user friendly manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the forward pass of the neural network is relatively simply as everything is basically just a series of matrix multiplicaitons. The example we looked at when working through the math had a forward pass which mathematically took the form \n",
    "$$\\text{softmax}((\\sigma(\\mathbf{x}W^{(1)})W^{(2)}))$$.\n",
    "We'll of course want to make things a bit more modular by allowing the user to choose some of the neural network paramters, such as the number of hidden layers `N_l` and the nunber of neurons per layer `L`. For simplicity we'll restrict that all the hidden layers are treated with sigmoid activaitons and the final layer processed with a softmax activation, which if you recall will be nessesary for us to ultimately purpose this MLP to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by simply defining our activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of `x`, calculated element-wise\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float or array_like\n",
    "        input\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sigmoid(x) : float or array_like\n",
    "        sigmoid applied to `x` element-wise\n",
    "    \"\"\"\n",
    "    return 1./(1.+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax of `x`,\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        (N x dim) array with N samples by p dimensions. dim=10 for MNIST classification. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    softmax(x) : float or array_like\n",
    "        softmax applied to `x` along the first axis.\n",
    "    \"\"\"\n",
    "    exponent = np.exp(x) # only compute the exponent once\n",
    "    return exponent/exponent.sum(axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs `x`, weight matricies `w`, and activations are in principle all we need to define the forward pass; however, for efficency reasons we'll want to store the outputs of the hidden layer neurons when performing the forward pass. Storing these values will help us later more quickly calculate the gradients during the backward pass. The `init_layers` functions will initalize these hidden layers as NumPy arrays, doing this before we begin training will help us save some overhead we would otherwise inccur reinitalizing these hidden layers before each forward pass. These hidden layer values will be stored in multi-dimensional matricies, called *tensors*. One dimension of these tensors will be the `batch size` which will indicate the number of samples simultaneously passed to MLP during one training loop (feed forward + backpropagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers(batch_size,layer_sizes):\n",
    "        \"\"\"\n",
    "        Initalize arrays to store the hidden layer ouputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            Number of samples to concurrently feed through the network.\n",
    "        layer_sizes : array_like\n",
    "            Array of length `N_l`. Each entry is the number of neurons in each layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        hidden_layers : list\n",
    "            List of empty arrays used to hold hidden layer outputs. \n",
    "        \"\"\" \n",
    "        hidden_layers = [np.empty((batch_size,layer_size)) for layer_size in layer_sizes]\n",
    "        return hidden_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform a forward pass our input `x` is consequtively multiplied by weight matricies passed into the associated activaiton functions. The paramters in these weight matricies will ultimately be learned through backpropagation, but each weight matrix must first be initalized to random values. There are a number of different methods for doing this initalization, but for the moment we'll use a simple approach of just drawing the numerical values from a normal distribution with mean zero and standard deviation 1. We could have also reasonably choosen to simply draw from a uniform distribution on the range `[-1,1]` (why would it be wrong to initalize the weight matricies with all zeros?). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(layer_sizes):\n",
    "        \"\"\"\n",
    "        Initalize the paramters of the weight matricies.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer_sizes : array_like\n",
    "            Array of length `N_l`. Each entry is the number of neurons in each layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weights : array_like\n",
    "            Randomly initalized weight matricies based on the layer sizes. \n",
    "        \"\"\"\n",
    "        weights = list()\n",
    "        for i in range(layer_sizes.shape[0]-1):\n",
    "            weights.append(np.random.uniform(-1,1,size=[layer_sizes[i],layer_sizes[i+1]]))\n",
    "        weights = asarray(self.weights)\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the `feed_forward` function to iterate though the calculations to perform the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(batch,hidden_layers,weights):\n",
    "    \"\"\"\n",
    "    Perform a forward pass of the neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch : array_like\n",
    "        (batch_size x dim) matrix of inputs\n",
    "    hidden_layers : list\n",
    "        List of hidden layer outputs\n",
    "    weights : array_like\n",
    "        Array of weight matricies\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output : array_like\n",
    "        Forward pass output of the MLP\n",
    "    hidden_layers : array_like\n",
    "        List of hidden layer outputs, populated from the forward pass.\n",
    "    \"\"\"\n",
    "    h_l = batch\n",
    "    hidden_layers[0] = h_l\n",
    "    for i,weight in enumerate(weights):\n",
    "        h_l = sigmoid(h_l.dot(weight))\n",
    "        hidden_layers[i+1]=h_l\n",
    "    output = softmax(hidden_layers[-1])\n",
    "    return output, hidden_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalizing all the weights, layers, and activations prior to the forward pass makes much of the backward pass implimentation actually quite simple. For convinience we'll define a `sigmoid_prime` function, which simply computes the derivative of the sigmoid activation $\\sigma^{\\prime}$. We'll use this when computing the gradients during the backward pass. Recall, $\\sigma^{\\prime}(x) = \\sigma(x)(1-\\sigma(x))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(sigmoid_out):\n",
    "    \"\"\"\n",
    "    Calculate derivative of sigmoid activation based on sigmoid output.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigmoid_out : array_like\n",
    "        Output values processed by a sigmoid function.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sigmoid_prime(h) : array_like\n",
    "        Derivative of sigmoid, based on value of sigmoid.\n",
    "    \"\"\"\n",
    "    return h*(1-h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With everything else in place computing we're finally ready to write the backpropagation algorithm. Again, the primary goal of this step is to update the parameters of the weight matricies using stochastic gradient descent on batches of training samples, measuring the error by comparing the outputs of neural network with the true labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(output,batch_y,hidden_layers,weights,batch_size,lr):\n",
    "        \"\"\"\n",
    "        Calculate derivative of sigmoid activation based on sigmoid output.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        output : array_like\n",
    "            Forward pass output of the MLP\n",
    "        batch_y : array_like\n",
    "            True labels for the samples in the batch\n",
    "        hidden_layers : list\n",
    "            List of hidden layer outputs  \n",
    "        weights : array_like\n",
    "            Array of weight matricies\n",
    "        lr : float\n",
    "            Learning rate for SGD\n",
    "        batch_size : int\n",
    "            Size of a training mini-batch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weights : array_like\n",
    "            Array of weight matricies, updated from the backpropagation.\n",
    "    \n",
    "        \"\"\"\n",
    "        delta_t = (output - batch_y)*sigmoid_prime(hidden_layers[-1])\n",
    "        for i in range(1,len(weights)+1):\n",
    "            weights[-i]-=lr*(hidden_layers[-i-1].T.dot(delta_t))/batch_size\n",
    "            delta_t = sigmoid_prime(hidden_layers[-i-1])*(delta_t.dot(weights[-i].T))\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined all the functions required to actually train the neural network, we're finally ready to assimilate everything into a training loop. The number of `epochs` we set defines the number of iterations we train our model. In each `epoch` a number of forward + backward pass are iteratively performed on mini-batches of data until our neural network has *seen* all the training data.\n",
    "\n",
    "A number of new functions such as `loss`, `accuracy`, and `to_categorical` are used in the `train` function below. These are used to record some metrics we'll display during learning and will be defined in full in our final implimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X,Y,layer_sizes,batch_size=8,epochs=25,lr=1.0):\n",
    "    \"\"\"\n",
    "    Train the MLP.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        Forward pass output of the MLP\n",
    "    Y : array_like\n",
    "        True labels for the samples in the batch\n",
    "    layer_sizes : \n",
    "        Array of length `N_l`. Each entry is the number of neurons in each layer\n",
    "    batch_size : int\n",
    "        Size of a training mini-batch\n",
    "    epochs : int\n",
    "        Number of iterations to train for\n",
    "    lr : float\n",
    "        Learning rate for SGD\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    weights : array_like\n",
    "        Array of weight matricies, updated from the backpropagation.\n",
    "\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    hidden_layers = init_layers(batch_size,layer_sizes)\n",
    "    weights = init_weights(layer_sizes)\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        shuffle = np.random.permutation(n_samples)       \n",
    "        X_batches = np.array_split(X[shuffle],n_samples/batch_size)\n",
    "        Y_batches = np.array_split(Y[shuffle],n_samples/batch_size)\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        \n",
    "        for batch_x,batch_y in zip(X_batches,Y_batches):\n",
    "            output,hidden_layers = feed_forward(batch_x,hidden_layers,weights)  \n",
    "            train_loss += loss(output,batch_y)\n",
    "            train_acc += accuracy(to_categorical(output),batch_y)\n",
    "            weights = back_prop(output,batch_y,hidden_layers,weights,batch_size,lr)\n",
    "\n",
    "        train_loss = (train_loss/len(X_batches))\n",
    "        train_acc = (train_acc/len(X_batches))\n",
    "\n",
    "        train_time = round(time.time()-start,3)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: loss = {train_loss.round(3)} | acc = {train_acc.round(3)} | train_time = {train_time} | tot_time = {tot_time}\")\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron: final form\n",
    "Everything that's been outlined suffices for running the MLP. However, as it stands things are still quite clunkly. We can clean everything up by wrapping all this code into a Python Class. If you're unfamiliar with object oriented programming in Python, I highly reccomend checking out this tutorial. \n",
    "\n",
    "Our original function defintions have been slightly changed to help absuse some of the properties of having a class structure. A number of new functions, such as `predict` and `evaluate`, have been defined in the `MLP` class. Carefully go through this code and try to understand exactly what's being done and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    \n",
    "    def __init__(self,X,Y,X_val,Y_val,L=1,N_l=128):\n",
    "        self.X = np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n",
    "        self.Y = np.squeeze(np.eye(27)[Y.astype(np.int).reshape(-1)])\n",
    "        self.X_val = np.concatenate((X_val,np.ones((X_val.shape[0],1))),axis=1)\n",
    "        self.Y_val = np.squeeze(np.eye(27)[Y_val.astype(np.int).reshape(-1)])\n",
    "        self.L = L\n",
    "        self.N_l = N_l\n",
    "        self.n_samples = self.X.shape[0]\n",
    "        self.layer_sizes =np.array([self.X.shape[1]]+[N_l]*L+[self.Y.shape[1]]) \n",
    "        self.__init_weights()\n",
    "        self.train_loss = list()\n",
    "        self.train_acc = list()\n",
    "        self.val_loss = list()\n",
    "        self.val_acc = list()\n",
    "        self.train_time = list()\n",
    "        self.tot_time = list()\n",
    "        self.metrics = [self.train_loss,self.train_acc,self.val_loss,self.val_acc,self.train_time,self.tot_time]\n",
    "        \n",
    "    def __sigmoid(self,x):\n",
    "        # VCompute the sigmoid\n",
    "        return 1./(1.+np.exp(-x))\n",
    "\n",
    "    def __tanh(self,x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def __softmax(self,x):\n",
    "        # Compute softmax along the rows of the input\n",
    "        exponent = np.exp(x)\n",
    "        return exponent/exponent.sum(axis=1,keepdims=True)\n",
    "    \n",
    "    def __loss(self,y_pred,y):\n",
    "        # Compute the loss along the rows, averaging along the number of samples\n",
    "        return ((-np.log(y_pred))*y).sum(axis=1).mean()\n",
    "    \n",
    "    def __accuracy(self,y_pred,y):  \n",
    "        # Compute the accuracy along the rows, averaging along the number of samples\n",
    "        return np.all(y_pred==y,axis=1).mean()\n",
    "    \n",
    "    def __sigmoid_prime(self,h):\n",
    "        # Compute the derivative of sigmoid where h=sigmoid(x)\n",
    "        return h*(1-h)\n",
    "    \n",
    "    def __tanh_prime(self,h):\n",
    "        return 1-h**2\n",
    "    \n",
    "    def __to_categorical(self,x):  \n",
    "        # Transform probabilities into categorical predictions row-wise, by simply taking the max probability\n",
    "        categorical = np.zeros((x.shape[0],self.Y.shape[1]))\n",
    "        categorical[np.arange(x.shape[0]),x.argmax(axis=1)] = 1\n",
    "        return categorical\n",
    "    \n",
    "    def __init_weights(self):\n",
    "        # Initialize the weights of the network given the sizes of the layers\n",
    "        self.weights = list()\n",
    "        for i in range(self.layer_sizes.shape[0]-1):\n",
    "            self.weights.append(np.random.uniform(-1,1,size=[self.layer_sizes[i],self.layer_sizes[i+1]]))\n",
    "        self.weights = np.asarray(self.weights)\n",
    "    \n",
    "    def __init_layers(self,batch_size):\n",
    "        # Initialize and allocate arrays for the hidden layer activations \n",
    "        self.__h = [np.empty((batch_size,layer)) for layer in self.layer_sizes]\n",
    "    \n",
    "    def __feed_forward(self,batch):\n",
    "        # Perform a forward pass of `batch` samples (N_samples x N_features)\n",
    "        h_l = batch\n",
    "        self.__h[0] = h_l\n",
    "        for i,weights in enumerate(self.weights):\n",
    "            h_l = self.__tanh(h_l.dot(weights))\n",
    "            self.__h[i+1]=h_l\n",
    "        self.__out = self.__softmax(self.__h[-1])\n",
    "    \n",
    "    def __back_prop(self,batch_y):\n",
    "        # Update the weights of the network through back-propagation\n",
    "        delta_t = (self.__out - batch_y)*self.__tanh_prime(self.__h[-1])\n",
    "        for i in range(1,len(self.weights)+1):\n",
    "            self.weights[-i]-=self.lr*(self.__h[-i-1].T.dot(delta_t))/self.batch_size\n",
    "            delta_t = self.__tanh_prime(self.__h[-i-1])*(delta_t.dot(self.weights[-i].T))\n",
    "            \n",
    "    def predict(self,X):\n",
    "        # Generate a categorical, one-hot, prediction given an input X\n",
    "        X = np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n",
    "        self.__init_layers(X.shape[0])\n",
    "        self.__feed_forward(X)\n",
    "        return self.__to_categorical(self.__out)\n",
    "    \n",
    "    def evaluate(self,X,Y):\n",
    "        # Evaluate the performance (accuracy) predicting on X with true labels Y\n",
    "        prediction = self.predict(X)\n",
    "        return self.__accuracy(prediction,Y)\n",
    "        \n",
    "    def train(self,batch_size=8,epochs=25,lr=1.0):\n",
    "        # Train the model with a given batch size, epochs, and learning rate. Store and print relevant metrics.\n",
    "        self.lr = lr\n",
    "        self.batch_size=batch_size\n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "            \n",
    "            self.__init_layers(self.batch_size)\n",
    "            shuffle = np.random.permutation(self.n_samples)\n",
    "            train_loss = 0\n",
    "            train_acc = 0\n",
    "            X_batches = np.array_split(self.X[shuffle],self.n_samples/self.batch_size)\n",
    "            Y_batches = np.array_split(self.Y[shuffle],self.n_samples/self.batch_size)\n",
    "            for batch_x,batch_y in zip(X_batches,Y_batches):\n",
    "                self.__feed_forward(batch_x)  \n",
    "                train_loss += self.__loss(self.__out,batch_y)\n",
    "                train_acc += self.__accuracy(self.__to_categorical(self.__out),batch_y)\n",
    "                self.__back_prop(batch_y)\n",
    "                \n",
    "            train_loss = (train_loss/len(X_batches))\n",
    "            train_acc = (train_acc/len(X_batches))\n",
    "            self.train_loss.append(train_loss)\n",
    "            self.train_acc.append(train_acc)\n",
    "            \n",
    "            train_time = round(time.time()-start,3)\n",
    "            self.train_time.append(train_time)\n",
    "            \n",
    "            self.__init_layers(self.X_val.shape[0])\n",
    "            self.__feed_forward(self.X_val)\n",
    "            val_loss = self.__loss(self.__out,self.Y_val)\n",
    "            val_acc = self.__accuracy(self.__to_categorical(self.__out),self.Y_val)\n",
    "            self.val_loss.append(val_loss)\n",
    "            self.val_acc.append(val_acc)\n",
    "            \n",
    "            tot_time = round(time.time()-start,3)\n",
    "            self.tot_time.append(tot_time)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: loss = {train_loss.round(3)} | acc = {train_acc.round(3)} | val_loss = {val_loss.round(3)} | val_acc = {val_acc.round(3)} | train_time = {train_time} | tot_time = {tot_time}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's give this a try. Let's create a really simple MLP with only a single hidden layer `L=1` with 128 neurons `N_l=128`. We'll train with a `batch_size=8` for `epochs=25` and a learning rate `lr=1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soustab\\AppData\\Local\\Temp\\ipykernel_4100\\477408485.py:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.Y = np.squeeze(np.eye(27)[Y.astype(np.int).reshape(-1)])\n",
      "C:\\Users\\Soustab\\AppData\\Local\\Temp\\ipykernel_4100\\477408485.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.Y_val = np.squeeze(np.eye(27)[Y_val.astype(np.int).reshape(-1)])\n",
      "C:\\Users\\Soustab\\AppData\\Local\\Temp\\ipykernel_4100\\477408485.py:59: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.weights = np.asarray(self.weights)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 2.939 | acc = 0.198 | val_loss = 2.549 | val_acc = 0.334 | train_time = 1.945 | tot_time = 1.953\n",
      "Epoch 2: loss = 2.418 | acc = 0.403 | val_loss = 2.378 | val_acc = 0.446 | train_time = 1.924 | tot_time = 1.931\n",
      "Epoch 3: loss = 2.273 | acc = 0.502 | val_loss = 2.305 | val_acc = 0.487 | train_time = 1.845 | tot_time = 1.853\n",
      "Epoch 4: loss = 2.205 | acc = 0.541 | val_loss = 2.251 | val_acc = 0.534 | train_time = 1.914 | tot_time = 1.92\n",
      "Epoch 5: loss = 2.145 | acc = 0.596 | val_loss = 2.265 | val_acc = 0.534 | train_time = 1.56 | tot_time = 1.567\n",
      "Epoch 6: loss = 2.118 | acc = 0.603 | val_loss = 2.229 | val_acc = 0.562 | train_time = 1.849 | tot_time = 1.855\n",
      "Epoch 7: loss = 2.087 | acc = 0.627 | val_loss = 2.208 | val_acc = 0.532 | train_time = 1.494 | tot_time = 1.501\n",
      "Epoch 8: loss = 2.045 | acc = 0.645 | val_loss = 2.167 | val_acc = 0.571 | train_time = 1.496 | tot_time = 1.502\n",
      "Epoch 9: loss = 2.022 | acc = 0.658 | val_loss = 2.165 | val_acc = 0.528 | train_time = 1.388 | tot_time = 1.394\n",
      "Epoch 10: loss = 2.003 | acc = 0.667 | val_loss = 2.106 | val_acc = 0.612 | train_time = 1.396 | tot_time = 1.402\n",
      "Epoch 11: loss = 1.98 | acc = 0.68 | val_loss = 2.111 | val_acc = 0.616 | train_time = 1.393 | tot_time = 1.399\n",
      "Epoch 12: loss = 1.969 | acc = 0.694 | val_loss = 2.127 | val_acc = 0.605 | train_time = 1.441 | tot_time = 1.447\n",
      "Epoch 13: loss = 1.956 | acc = 0.7 | val_loss = 2.084 | val_acc = 0.628 | train_time = 1.417 | tot_time = 1.423\n",
      "Epoch 14: loss = 1.933 | acc = 0.725 | val_loss = 2.055 | val_acc = 0.662 | train_time = 1.4 | tot_time = 1.406\n",
      "Epoch 15: loss = 1.933 | acc = 0.738 | val_loss = 2.125 | val_acc = 0.642 | train_time = 1.396 | tot_time = 1.403\n",
      "Epoch 16: loss = 1.904 | acc = 0.751 | val_loss = 2.059 | val_acc = 0.69 | train_time = 1.401 | tot_time = 1.407\n",
      "Epoch 17: loss = 1.894 | acc = 0.753 | val_loss = 2.068 | val_acc = 0.659 | train_time = 1.376 | tot_time = 1.382\n",
      "Epoch 18: loss = 1.888 | acc = 0.754 | val_loss = 2.067 | val_acc = 0.655 | train_time = 1.339 | tot_time = 1.345\n",
      "Epoch 19: loss = 1.877 | acc = 0.762 | val_loss = 2.062 | val_acc = 0.67 | train_time = 1.474 | tot_time = 1.48\n",
      "Epoch 20: loss = 1.858 | acc = 0.774 | val_loss = 2.081 | val_acc = 0.658 | train_time = 1.379 | tot_time = 1.385\n",
      "Epoch 21: loss = 1.86 | acc = 0.772 | val_loss = 2.051 | val_acc = 0.662 | train_time = 1.388 | tot_time = 1.395\n",
      "Epoch 22: loss = 1.847 | acc = 0.779 | val_loss = 2.068 | val_acc = 0.668 | train_time = 1.35 | tot_time = 1.357\n",
      "Epoch 23: loss = 1.84 | acc = 0.791 | val_loss = 2.051 | val_acc = 0.671 | train_time = 1.49 | tot_time = 1.498\n",
      "Epoch 24: loss = 1.83 | acc = 0.793 | val_loss = 2.036 | val_acc = 0.682 | train_time = 1.47 | tot_time = 1.477\n",
      "Epoch 25: loss = 1.817 | acc = 0.809 | val_loss = 2.059 | val_acc = 0.695 | train_time = 1.426 | tot_time = 1.433\n",
      "Epoch 26: loss = 1.82 | acc = 0.803 | val_loss = 2.047 | val_acc = 0.708 | train_time = 1.386 | tot_time = 1.392\n",
      "Epoch 27: loss = 1.799 | acc = 0.813 | val_loss = 2.052 | val_acc = 0.72 | train_time = 1.403 | tot_time = 1.409\n",
      "Epoch 28: loss = 1.801 | acc = 0.816 | val_loss = 2.057 | val_acc = 0.67 | train_time = 1.429 | tot_time = 1.435\n",
      "Epoch 29: loss = 1.805 | acc = 0.815 | val_loss = 2.053 | val_acc = 0.697 | train_time = 1.422 | tot_time = 1.429\n",
      "Epoch 30: loss = 1.792 | acc = 0.817 | val_loss = 2.097 | val_acc = 0.623 | train_time = 1.411 | tot_time = 1.419\n",
      "Epoch 31: loss = 1.79 | acc = 0.817 | val_loss = 2.049 | val_acc = 0.682 | train_time = 1.392 | tot_time = 1.398\n",
      "Epoch 32: loss = 1.78 | acc = 0.83 | val_loss = 2.06 | val_acc = 0.716 | train_time = 1.401 | tot_time = 1.408\n",
      "Epoch 33: loss = 1.772 | acc = 0.835 | val_loss = 2.084 | val_acc = 0.705 | train_time = 1.402 | tot_time = 1.408\n",
      "Epoch 34: loss = 1.783 | acc = 0.825 | val_loss = 2.042 | val_acc = 0.681 | train_time = 1.477 | tot_time = 1.484\n",
      "Epoch 35: loss = 1.773 | acc = 0.837 | val_loss = 2.052 | val_acc = 0.71 | train_time = 1.477 | tot_time = 1.484\n",
      "Epoch 36: loss = 1.76 | acc = 0.845 | val_loss = 2.066 | val_acc = 0.732 | train_time = 1.507 | tot_time = 1.513\n",
      "Epoch 37: loss = 1.753 | acc = 0.847 | val_loss = 2.018 | val_acc = 0.721 | train_time = 1.411 | tot_time = 1.419\n",
      "Epoch 38: loss = 1.754 | acc = 0.85 | val_loss = 2.038 | val_acc = 0.712 | train_time = 1.404 | tot_time = 1.411\n",
      "Epoch 39: loss = 1.748 | acc = 0.853 | val_loss = 2.022 | val_acc = 0.728 | train_time = 1.534 | tot_time = 1.541\n",
      "Epoch 40: loss = 1.744 | acc = 0.853 | val_loss = 2.064 | val_acc = 0.709 | train_time = 1.475 | tot_time = 1.482\n",
      "Epoch 41: loss = 1.756 | acc = 0.84 | val_loss = 2.062 | val_acc = 0.721 | train_time = 1.439 | tot_time = 1.446\n",
      "Epoch 42: loss = 1.745 | acc = 0.855 | val_loss = 2.08 | val_acc = 0.724 | train_time = 1.372 | tot_time = 1.379\n",
      "Epoch 43: loss = 1.742 | acc = 0.856 | val_loss = 2.065 | val_acc = 0.705 | train_time = 1.554 | tot_time = 1.56\n",
      "Epoch 44: loss = 1.744 | acc = 0.852 | val_loss = 2.012 | val_acc = 0.72 | train_time = 1.513 | tot_time = 1.521\n",
      "Epoch 45: loss = 1.737 | acc = 0.856 | val_loss = 2.011 | val_acc = 0.73 | train_time = 1.496 | tot_time = 1.502\n",
      "Epoch 46: loss = 1.728 | acc = 0.865 | val_loss = 2.036 | val_acc = 0.74 | train_time = 1.387 | tot_time = 1.394\n",
      "Epoch 47: loss = 1.726 | acc = 0.864 | val_loss = 2.039 | val_acc = 0.728 | train_time = 1.392 | tot_time = 1.398\n",
      "Epoch 48: loss = 1.726 | acc = 0.863 | val_loss = 2.023 | val_acc = 0.725 | train_time = 1.383 | tot_time = 1.389\n",
      "Epoch 49: loss = 1.723 | acc = 0.867 | val_loss = 2.041 | val_acc = 0.751 | train_time = 1.346 | tot_time = 1.353\n",
      "Epoch 50: loss = 1.723 | acc = 0.867 | val_loss = 2.027 | val_acc = 0.732 | train_time = 1.403 | tot_time = 1.41\n",
      "Epoch 51: loss = 1.72 | acc = 0.872 | val_loss = 2.009 | val_acc = 0.745 | train_time = 1.418 | tot_time = 1.425\n",
      "Epoch 52: loss = 1.723 | acc = 0.869 | val_loss = 2.011 | val_acc = 0.739 | train_time = 1.457 | tot_time = 1.464\n",
      "Epoch 53: loss = 1.716 | acc = 0.876 | val_loss = 2.039 | val_acc = 0.726 | train_time = 1.38 | tot_time = 1.386\n",
      "Epoch 54: loss = 1.72 | acc = 0.869 | val_loss = 1.996 | val_acc = 0.757 | train_time = 1.382 | tot_time = 1.388\n",
      "Epoch 55: loss = 1.718 | acc = 0.871 | val_loss = 2.043 | val_acc = 0.712 | train_time = 1.379 | tot_time = 1.386\n",
      "Epoch 56: loss = 1.718 | acc = 0.873 | val_loss = 2.037 | val_acc = 0.747 | train_time = 1.38 | tot_time = 1.387\n",
      "Epoch 57: loss = 1.714 | acc = 0.875 | val_loss = 2.03 | val_acc = 0.743 | train_time = 1.383 | tot_time = 1.389\n",
      "Epoch 58: loss = 1.712 | acc = 0.878 | val_loss = 2.021 | val_acc = 0.753 | train_time = 1.448 | tot_time = 1.454\n",
      "Epoch 59: loss = 1.706 | acc = 0.882 | val_loss = 2.027 | val_acc = 0.739 | train_time = 1.428 | tot_time = 1.434\n",
      "Epoch 60: loss = 1.702 | acc = 0.882 | val_loss = 2.044 | val_acc = 0.741 | train_time = 1.423 | tot_time = 1.43\n",
      "Epoch 61: loss = 1.702 | acc = 0.884 | val_loss = 2.042 | val_acc = 0.735 | train_time = 1.337 | tot_time = 1.343\n",
      "Epoch 62: loss = 1.699 | acc = 0.881 | val_loss = 2.066 | val_acc = 0.737 | train_time = 1.343 | tot_time = 1.348\n",
      "Epoch 63: loss = 1.7 | acc = 0.884 | val_loss = 2.041 | val_acc = 0.739 | train_time = 1.343 | tot_time = 1.349\n",
      "Epoch 64: loss = 1.699 | acc = 0.884 | val_loss = 2.048 | val_acc = 0.74 | train_time = 1.39 | tot_time = 1.395\n",
      "Epoch 65: loss = 1.7 | acc = 0.889 | val_loss = 2.043 | val_acc = 0.733 | train_time = 1.334 | tot_time = 1.341\n",
      "Epoch 66: loss = 1.698 | acc = 0.886 | val_loss = 2.07 | val_acc = 0.744 | train_time = 1.353 | tot_time = 1.359\n",
      "Epoch 67: loss = 1.693 | acc = 0.889 | val_loss = 2.038 | val_acc = 0.747 | train_time = 1.387 | tot_time = 1.393\n",
      "Epoch 68: loss = 1.688 | acc = 0.889 | val_loss = 2.062 | val_acc = 0.745 | train_time = 1.511 | tot_time = 1.517\n",
      "Epoch 69: loss = 1.687 | acc = 0.892 | val_loss = 2.072 | val_acc = 0.753 | train_time = 1.388 | tot_time = 1.394\n",
      "Epoch 70: loss = 1.685 | acc = 0.893 | val_loss = 2.05 | val_acc = 0.739 | train_time = 1.373 | tot_time = 1.379\n",
      "Epoch 71: loss = 1.685 | acc = 0.891 | val_loss = 2.049 | val_acc = 0.73 | train_time = 1.33 | tot_time = 1.336\n",
      "Epoch 72: loss = 1.687 | acc = 0.895 | val_loss = 2.026 | val_acc = 0.744 | train_time = 1.347 | tot_time = 1.353\n",
      "Epoch 73: loss = 1.688 | acc = 0.893 | val_loss = 2.056 | val_acc = 0.729 | train_time = 1.373 | tot_time = 1.38\n",
      "Epoch 74: loss = 1.687 | acc = 0.893 | val_loss = 2.066 | val_acc = 0.73 | train_time = 1.402 | tot_time = 1.409\n",
      "Epoch 75: loss = 1.682 | acc = 0.893 | val_loss = 2.075 | val_acc = 0.748 | train_time = 1.321 | tot_time = 1.327\n",
      "Epoch 76: loss = 1.681 | acc = 0.895 | val_loss = 2.087 | val_acc = 0.729 | train_time = 1.344 | tot_time = 1.351\n",
      "Epoch 77: loss = 1.68 | acc = 0.897 | val_loss = 2.075 | val_acc = 0.739 | train_time = 1.335 | tot_time = 1.341\n",
      "Epoch 78: loss = 1.68 | acc = 0.895 | val_loss = 2.065 | val_acc = 0.737 | train_time = 1.345 | tot_time = 1.351\n",
      "Epoch 79: loss = 1.679 | acc = 0.897 | val_loss = 2.053 | val_acc = 0.755 | train_time = 1.356 | tot_time = 1.362\n",
      "Epoch 80: loss = 1.677 | acc = 0.897 | val_loss = 2.058 | val_acc = 0.753 | train_time = 1.359 | tot_time = 1.366\n",
      "Epoch 81: loss = 1.677 | acc = 0.898 | val_loss = 2.058 | val_acc = 0.755 | train_time = 1.367 | tot_time = 1.373\n",
      "Epoch 82: loss = 1.676 | acc = 0.898 | val_loss = 2.088 | val_acc = 0.751 | train_time = 1.328 | tot_time = 1.334\n",
      "Epoch 83: loss = 1.676 | acc = 0.899 | val_loss = 2.088 | val_acc = 0.753 | train_time = 1.335 | tot_time = 1.342\n",
      "Epoch 84: loss = 1.676 | acc = 0.898 | val_loss = 2.08 | val_acc = 0.747 | train_time = 1.468 | tot_time = 1.475\n",
      "Epoch 85: loss = 1.673 | acc = 0.899 | val_loss = 2.078 | val_acc = 0.755 | train_time = 1.474 | tot_time = 1.48\n",
      "Epoch 86: loss = 1.672 | acc = 0.901 | val_loss = 2.084 | val_acc = 0.749 | train_time = 1.436 | tot_time = 1.442\n",
      "Epoch 87: loss = 1.676 | acc = 0.899 | val_loss = 2.087 | val_acc = 0.748 | train_time = 1.41 | tot_time = 1.417\n",
      "Epoch 88: loss = 1.675 | acc = 0.9 | val_loss = 2.084 | val_acc = 0.745 | train_time = 1.391 | tot_time = 1.396\n",
      "Epoch 89: loss = 1.673 | acc = 0.898 | val_loss = 2.076 | val_acc = 0.747 | train_time = 1.396 | tot_time = 1.403\n",
      "Epoch 90: loss = 1.672 | acc = 0.9 | val_loss = 2.083 | val_acc = 0.749 | train_time = 1.458 | tot_time = 1.464\n",
      "Epoch 91: loss = 1.672 | acc = 0.899 | val_loss = 2.088 | val_acc = 0.745 | train_time = 1.421 | tot_time = 1.427\n",
      "Epoch 92: loss = 1.672 | acc = 0.9 | val_loss = 2.093 | val_acc = 0.747 | train_time = 1.396 | tot_time = 1.402\n",
      "Epoch 93: loss = 1.672 | acc = 0.9 | val_loss = 2.095 | val_acc = 0.749 | train_time = 1.392 | tot_time = 1.399\n",
      "Epoch 94: loss = 1.672 | acc = 0.901 | val_loss = 2.097 | val_acc = 0.751 | train_time = 1.418 | tot_time = 1.424\n",
      "Epoch 95: loss = 1.671 | acc = 0.901 | val_loss = 2.097 | val_acc = 0.751 | train_time = 1.378 | tot_time = 1.384\n",
      "Epoch 96: loss = 1.671 | acc = 0.901 | val_loss = 2.096 | val_acc = 0.751 | train_time = 1.422 | tot_time = 1.428\n",
      "Epoch 97: loss = 1.671 | acc = 0.901 | val_loss = 2.096 | val_acc = 0.751 | train_time = 1.433 | tot_time = 1.439\n",
      "Epoch 98: loss = 1.671 | acc = 0.901 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.393 | tot_time = 1.4\n",
      "Epoch 99: loss = 1.671 | acc = 0.901 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.4 | tot_time = 1.406\n",
      "Epoch 100: loss = 1.671 | acc = 0.901 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.446 | tot_time = 1.453\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X_train,Y_train,X_test,Y_test,L=1,N_l=256)\n",
    "model.train(batch_size=8,epochs=100,lr=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, along with the training data, `X_train` and `Y_train`, we include a validation dataset, `X_val` and `Y_val`. The purpose of this data is to evaluate the generizability of our model. We expect our model to perform well on our training data, because of course the objective of our optimization is to minimize the error with respect to the training data, but we'd like our model to generalize to new, never before seen, data. We therefore evaluate the accuracy and loss on a hold-out set which the model never sees during training. If the performance is good on this hold-out set we can be confident that our model is generalizing well, meaning we've mananged to generally teach a computer to read hand-writing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soustab\\AppData\\Local\\Temp\\ipykernel_4100\\477408485.py:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.Y = np.squeeze(np.eye(27)[Y.astype(np.int).reshape(-1)])\n",
      "C:\\Users\\Soustab\\AppData\\Local\\Temp\\ipykernel_4100\\477408485.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.Y_val = np.squeeze(np.eye(27)[Y_val.astype(np.int).reshape(-1)])\n",
      "C:\\Users\\Soustab\\AppData\\Local\\Temp\\ipykernel_4100\\477408485.py:59: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.weights = np.asarray(self.weights)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 1.671 | acc = 0.901 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.576 | tot_time = 1.582\n",
      "Epoch 2: loss = 1.671 | acc = 0.901 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.441 | tot_time = 1.449\n",
      "Epoch 3: loss = 1.671 | acc = 0.901 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.336 | tot_time = 1.342\n",
      "Epoch 4: loss = 1.671 | acc = 0.901 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.413 | tot_time = 1.419\n",
      "Epoch 5: loss = 1.671 | acc = 0.901 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.349 | tot_time = 1.355\n",
      "Epoch 6: loss = 1.671 | acc = 0.901 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.404 | tot_time = 1.411\n",
      "Epoch 7: loss = 1.671 | acc = 0.901 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.381 | tot_time = 1.387\n",
      "Epoch 8: loss = 1.671 | acc = 0.901 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.353 | tot_time = 1.359\n",
      "Epoch 9: loss = 1.671 | acc = 0.9 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.379 | tot_time = 1.386\n",
      "Epoch 10: loss = 1.671 | acc = 0.9 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.378 | tot_time = 1.384\n",
      "Epoch 11: loss = 1.671 | acc = 0.9 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.376 | tot_time = 1.383\n",
      "Epoch 12: loss = 1.671 | acc = 0.901 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.396 | tot_time = 1.403\n",
      "Epoch 13: loss = 1.671 | acc = 0.9 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.467 | tot_time = 1.474\n",
      "Epoch 14: loss = 1.671 | acc = 0.9 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.413 | tot_time = 1.419\n",
      "Epoch 15: loss = 1.671 | acc = 0.9 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.374 | tot_time = 1.38\n",
      "Epoch 16: loss = 1.671 | acc = 0.9 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.541 | tot_time = 1.547\n",
      "Epoch 17: loss = 1.671 | acc = 0.9 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.497 | tot_time = 1.503\n",
      "Epoch 18: loss = 1.671 | acc = 0.901 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.435 | tot_time = 1.44\n",
      "Epoch 19: loss = 1.671 | acc = 0.9 | val_loss = 2.095 | val_acc = 0.751 | train_time = 1.494 | tot_time = 1.502\n",
      "Epoch 20: loss = 1.671 | acc = 0.901 | val_loss = 2.096 | val_acc = 0.751 | train_time = 1.431 | tot_time = 1.438\n",
      "Epoch 21: loss = 1.671 | acc = 0.901 | val_loss = 2.096 | val_acc = 0.749 | train_time = 1.356 | tot_time = 1.363\n",
      "Epoch 22: loss = 1.671 | acc = 0.901 | val_loss = 2.096 | val_acc = 0.749 | train_time = 1.324 | tot_time = 1.329\n",
      "Epoch 23: loss = 1.671 | acc = 0.901 | val_loss = 2.096 | val_acc = 0.749 | train_time = 1.35 | tot_time = 1.355\n",
      "Epoch 24: loss = 1.671 | acc = 0.901 | val_loss = 2.096 | val_acc = 0.749 | train_time = 1.418 | tot_time = 1.424\n",
      "Epoch 25: loss = 1.671 | acc = 0.9 | val_loss = 2.096 | val_acc = 0.749 | train_time = 1.338 | tot_time = 1.345\n",
      "Epoch 26: loss = 1.671 | acc = 0.901 | val_loss = 2.096 | val_acc = 0.749 | train_time = 1.402 | tot_time = 1.409\n",
      "Epoch 27: loss = 1.671 | acc = 0.901 | val_loss = 2.096 | val_acc = 0.748 | train_time = 1.358 | tot_time = 1.364\n",
      "Epoch 28: loss = 1.671 | acc = 0.901 | val_loss = 2.096 | val_acc = 0.748 | train_time = 1.379 | tot_time = 1.386\n",
      "Epoch 29: loss = 1.671 | acc = 0.901 | val_loss = 2.096 | val_acc = 0.748 | train_time = 1.34 | tot_time = 1.347\n",
      "Epoch 30: loss = 1.671 | acc = 0.901 | val_loss = 2.096 | val_acc = 0.748 | train_time = 1.343 | tot_time = 1.35\n",
      "Epoch 31: loss = 1.671 | acc = 0.901 | val_loss = 2.096 | val_acc = 0.748 | train_time = 1.307 | tot_time = 1.314\n",
      "Epoch 32: loss = 1.671 | acc = 0.901 | val_loss = 2.096 | val_acc = 0.748 | train_time = 1.36 | tot_time = 1.367\n",
      "Epoch 33: loss = 1.671 | acc = 0.901 | val_loss = 2.096 | val_acc = 0.748 | train_time = 1.411 | tot_time = 1.418\n",
      "Epoch 34: loss = 1.671 | acc = 0.901 | val_loss = 2.096 | val_acc = 0.748 | train_time = 1.421 | tot_time = 1.428\n",
      "Epoch 35: loss = 1.671 | acc = 0.901 | val_loss = 2.096 | val_acc = 0.748 | train_time = 1.383 | tot_time = 1.389\n",
      "Epoch 36: loss = 1.671 | acc = 0.901 | val_loss = 2.096 | val_acc = 0.748 | train_time = 1.328 | tot_time = 1.334\n",
      "Epoch 37: loss = 1.671 | acc = 0.901 | val_loss = 2.096 | val_acc = 0.748 | train_time = 1.346 | tot_time = 1.353\n",
      "Epoch 38: loss = 1.671 | acc = 0.9 | val_loss = 2.096 | val_acc = 0.748 | train_time = 1.333 | tot_time = 1.338\n",
      "Epoch 39: loss = 1.671 | acc = 0.901 | val_loss = 2.096 | val_acc = 0.748 | train_time = 1.331 | tot_time = 1.338\n",
      "Epoch 40: loss = 1.671 | acc = 0.9 | val_loss = 2.096 | val_acc = 0.748 | train_time = 1.474 | tot_time = 1.481\n"
     ]
    }
   ],
   "source": [
    "model2 = MLP(X_train,Y_train,X_test,Y_test,L=1,N_l=256)\n",
    "model.train(batch_size=8,epochs=40,lr=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "ax[0].plot(model.train_loss,label=\"Train loss\")\n",
    "ax[0].plot(model.val_loss,label=\"Val loss\")\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].plot(model.train_acc,label=\"Train acc\")\n",
    "ax[1].plot(model.val_acc,label=\"Val acc\")\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the results here are impressive, the *hyperparamters* were carefully currated to achieve this good performance. The *hyperparamters* reffer to the paramters of the model which are not learned during the training loop (e.g. the `batch_size`, learning rate `lr`, number of hidden layers `N_l`, number of neurons-per-layer `L`). In general, these paramters need to be carefully tuned depending on the details and idiosycrisies of each problem. This tuning process tpyically involves training a model to completion under a number of different hyperparamter settings and selecting the combination of hyperparamters which yields the highest performance (measured in this case by the accuracy on our training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7479784366576819"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.731891947550706\n",
      "Recall: 0.7479784366576819\n",
      "Precision: 0.7227769094445233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soustab\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred=(np.argmax(y_pred, axis=1)+1).reshape(-1, 1).flatten()\n",
    "y_test_c=(np.argmax(y_test, axis=1)+1).reshape(-1, 1).flatten()\n",
    "# print(y_test_c)\n",
    "print ('F1 score:', f1_score(y_test_c, y_pred,average='weighted'))\n",
    "print ('Recall:', recall_score(y_test_c, y_pred,average='weighted'))\n",
    "print ('Precision:', precision_score(y_test_c, y_pred,average='weighted'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
